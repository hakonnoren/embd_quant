{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary embeddings with [mixedbread-ai/mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n",
    "\n",
    "Our model was trained to have a non-'clunky' embeddings space. This allows for quantizing the embeddings with low performance loss compared to techniques like Matryoshka. With binary embeddings, we can use the Hamming distance, which is well optimized for CPUs.\n",
    "\n",
    "In general, the approach is divided into 2 steps:\n",
    "\n",
    "1. Retrieve candidates based on Hamming distance.\n",
    "2. Rescore the candidates based on the dot product between the binary embedding and the floating embedding of the query.\n",
    "\n",
    "We find that we can retain ~96-99% of the performance, achieve ~40x faster retrieval, and realize 32x storage savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import faiss\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the worlds best model xD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No model needed — we load cached embeddings\n",
    "from pathlib import Path\n",
    "CACHE_DIR = Path(\"cache/embeddings\")\n",
    "model_short = \"mxbai-embed-large-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrecCovid is a nice benchmark, not too large, not too small, also pretty difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data_loader to ensure IDs match cached embedding order\n",
    "from data_loader import MTEBDataLoader\n",
    "\n",
    "# Switch between (\"scifact\", \"SciFact\"), (\"nfcorpus\", \"NFCorpus\"), (\"fiqa\", \"fiqa\")\n",
    "dataset_name = \"fiqa\"\n",
    "dataset_cache_name = \"fiqa\"  # must match cache filename casing\n",
    "\n",
    "truncate_dim = 128\n",
    "\n",
    "\n",
    "\n",
    "data_loader = MTEBDataLoader(Path(\"cache/datasets\"))\n",
    "corpus, queries_dict, qrels = data_loader.load_dataset(dataset_name)\n",
    "docs_ids, doc_texts, all_query_ids, query_texts = data_loader.get_texts_for_embedding(corpus, queries_dict)\n",
    "\n",
    "# Load cached embeddings (same order as docs_ids / all_query_ids)\n",
    "emb = np.load(CACHE_DIR / f\"{model_short}_{dataset_cache_name}_corpus.npy\")\n",
    "assert emb.shape[0] == len(docs_ids), f\"Corpus mismatch: {emb.shape[0]} embeddings vs {len(docs_ids)} doc IDs\"\n",
    "print(f\"Loaded {dataset_name}: {len(docs_ids)} docs ({emb.shape}), {len(all_query_ids)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "faiss.write_index(index, \"index_fp32.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_truncated = emb[:,:truncate_dim]\n",
    "\n",
    "bemb = np.packbits(emb_truncated > 0).reshape(emb_truncated.shape[0], -1)\n",
    "print(\"Binary embeddings computed. Shape:\", bemb.shape)\n",
    "num_dim = emb_truncated.shape[1]\n",
    "bindex = faiss.IndexBinaryFlat(num_dim)\n",
    "bindex.add(bemb)\n",
    "faiss.write_index_binary(bindex, \"index_binary.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check file size\n",
    "fp32_index_size = os.path.getsize(\"index_fp32.faiss\")\n",
    "binary_index_size = os.path.getsize(\"index_binary.faiss\")\n",
    "print(\"File size of index_fp32.faiss:\", fp32_index_size)\n",
    "print(\"File size of index_binary.faiss:\", binary_index_size)\n",
    "print(\"Compression ratio:\", fp32_index_size / binary_index_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid, rels in qrels.items():\n",
    "    pos = {did: score for did, score in rels.items() if score > 0}\n",
    "    print(pos)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some BEIR stuff for the eval later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qrels already loaded by data_loader above — filter to positive scores\n",
    "# (data_loader includes 0-relevance entries for some datasets)\n",
    "qrels_pos = {}\n",
    "for qid, rels in qrels.items():\n",
    "    pos = {did: score for did, score in rels.items() if score > 0}\n",
    "    if pos:\n",
    "        qrels_pos[qid] = pos\n",
    "qrels = qrels_pos\n",
    "print(f\"Qrels: {len(qrels)} queries with positive relevance judgments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to queries with qrels, keeping aligned indices for embedding slicing\n",
    "keep_mask = [qid in qrels for qid in all_query_ids]\n",
    "query_ids = [qid for qid, keep in zip(all_query_ids, keep_mask) if keep]\n",
    "print(f\"Queries: {len(all_query_ids)} total → {len(query_ids)} with positive qrels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cached query embeddings and filter to queries with qrels\n",
    "all_query_emb = np.load(CACHE_DIR / f\"{model_short}_{dataset_cache_name}_queries.npy\")\n",
    "assert all_query_emb.shape[0] == len(all_query_ids), f\"Query mismatch: {all_query_emb.shape[0]} vs {len(all_query_ids)}\"\n",
    "query_emb = all_query_emb[keep_mask]\n",
    "\n",
    "query_truncated = query_emb[:,:truncate_dim]\n",
    "\n",
    "query_bemb = np.packbits(query_truncated > 0).reshape(query_truncated.shape[0], -1)\n",
    "print(f\"Query embeddings: {all_query_emb.shape} → {query_emb.shape} (with qrels), binary: {query_bemb.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import RetrievalMetrics\n",
    "\n",
    "def faiss_search(index, queries_emb, k=[10, 100],float_embed=None, float_q_embed=None, oversample=1):\n",
    "    start_time = time.time()\n",
    "    faiss_scores, faiss_doc_ids = index.search(queries_emb, max(k) * oversample)\n",
    "    print(f\"Search took {(time.time()-start_time):.4f} sec\")\n",
    "\n",
    "    query2id = {idx: qid for idx, qid in enumerate(query_ids)}\n",
    "    doc2id = {idx: cid for idx, cid in enumerate(docs_ids)}\n",
    "    id2doc = {cid: idx for idx, cid in enumerate(docs_ids)}\n",
    "\n",
    "    # Build per-query result dicts (for display) and final indices array (for metrics)\n",
    "    if float_q_embed is not None:\n",
    "        # Rescore: reorder candidates by float query × unpacked binary ±1\n",
    "        n_q = len(queries_emb)\n",
    "        final_k = max(k)\n",
    "        final_indices = np.zeros((n_q, final_k), dtype=np.int64)\n",
    "        for idx in range(n_q):\n",
    "            cand_idx = faiss_doc_ids[idx]\n",
    "\n",
    "            if  float_embed is not None:\n",
    "                doc_emb = float_embed[cand_idx]\n",
    "            else:\n",
    "                bin_doc_emb = np.asarray([index.reconstruct(int(did)) for did in cand_idx])\n",
    "                bin_doc_emb_unpacked = np.unpackbits(bin_doc_emb, axis=-1).astype(np.float32)\n",
    "                doc_emb = bin_doc_emb_unpacked\n",
    "\n",
    "    \n",
    "            scores_cont = float_q_embed[idx] @ doc_emb.T\n",
    "            reranked = np.argsort(-scores_cont)[:final_k]\n",
    "            final_indices[idx] = cand_idx[reranked]\n",
    "    else:\n",
    "        final_indices = faiss_doc_ids[:, :max(k)]\n",
    "\n",
    "    # Compute metrics using our own module\n",
    "    doc_id_to_idx = {cid: idx for idx, cid in enumerate(docs_ids)}\n",
    "    metrics = RetrievalMetrics.compute_all_metrics(final_indices, qrels, doc_id_to_idx, query_ids, k)\n",
    "    for name, val in sorted(metrics.items()):\n",
    "        print(f\"  {name}: {val:.4f}\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_search(index, query_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W/O Rescoring\n",
    "\n",
    "We loose around 53% of the performance. But its pretty fast ~30-40x faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_search(bindex, query_bemb, oversample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_search(bindex, query_bemb,float_embed=emb, float_q_embed=query_emb, oversample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binary_quant_rerank import *\n",
    "cfg = ExperimentConfig()\n",
    "run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Binary embedding enables extremely fast retrieval and low storage usage, at the expense of a slight performance loss, which can be mitigated by using a reranker. This has cool applications for on-device usage, large-scale developments, etc. We should also explore its potential for other tasks, such as clustering and deduplication at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
