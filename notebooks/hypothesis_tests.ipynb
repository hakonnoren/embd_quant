{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac869c07",
   "metadata": {},
   "source": [
    "# Hypothesis Testing: Improving Embedding Quantization for Vector Search\n",
    "\n",
    "This notebook tests specific hypotheses from the reasoning context:\n",
    "- **Q2**: Per-dimension weighting for asymmetric binary search\n",
    "- **Q3**: 2-bit (ternary & quaternary) quantization\n",
    "- **Q4**: Optimal thresholds beyond median\n",
    "- **Q5**: Why does median hurt at full dimension?\n",
    "- **Q6**: Mixed-precision Matryoshka quantization\n",
    "- **Q7**: Information-theoretic bounds\n",
    "- **Q8**: Pareto front visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f53f1",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path(\"/Users/haakno/Documents/Research work/vespa/quantization_tests\")\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from data_loader import MTEBDataLoader\n",
    "from metrics import RetrievalMetrics\n",
    "from config import MODELS, DATASETS, K_VALUES\n",
    "\n",
    "# Cache dirs\n",
    "CACHE_DIR = PROJECT_ROOT / \"cache\"\n",
    "EMBEDDING_DIR = CACHE_DIR / \"embeddings\"\n",
    "\n",
    "# ─── Helper functions ───\n",
    "def load_embeddings(model_key: str, dataset: str):\n",
    "    \"\"\"Load cached corpus and query embeddings.\"\"\"\n",
    "    corpus_path = EMBEDDING_DIR / f\"{model_key}_{dataset}_corpus.npy\"\n",
    "    query_path = EMBEDDING_DIR / f\"{model_key}_{dataset}_queries.npy\"\n",
    "    corpus_emb = np.load(corpus_path)\n",
    "    query_emb = np.load(query_path)\n",
    "    return corpus_emb.astype(np.float32), query_emb.astype(np.float32)\n",
    "\n",
    "def truncate_and_normalize(emb: np.ndarray, dim: int) -> np.ndarray:\n",
    "    \"\"\"Matryoshka truncation + L2 normalization.\"\"\"\n",
    "    t = emb[:, :dim].copy()\n",
    "    norms = np.linalg.norm(t, axis=1, keepdims=True)\n",
    "    norms = np.where(norms == 0, 1.0, norms)\n",
    "    return t / norms\n",
    "\n",
    "def packbits(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Binarize (x > 0) and pack into uint8.\"\"\"\n",
    "    return np.packbits((x > 0).astype(np.uint8), axis=1)\n",
    "\n",
    "def unpack_to_pm1(packed: np.ndarray, dim: int) -> np.ndarray:\n",
    "    \"\"\"Unpack binary to {-1, +1} float32.\"\"\"\n",
    "    unpacked = np.unpackbits(packed, axis=1)[:, :dim].astype(np.float32)\n",
    "    return 2.0 * unpacked - 1.0\n",
    "\n",
    "def search_topk(scores: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"Get top-k indices from similarity scores (higher = better).\"\"\"\n",
    "    k = min(k, scores.shape[1])\n",
    "    indices = np.argpartition(-scores, k, axis=1)[:, :k]\n",
    "    for i in range(len(indices)):\n",
    "        idx = indices[i]\n",
    "        indices[i] = idx[np.argsort(-scores[i, idx])]\n",
    "    return indices\n",
    "\n",
    "def evaluate_indices(indices: np.ndarray, qrels: Dict, doc_id_to_idx: Dict, \n",
    "                     query_ids: List[str], k: int = 10) -> Dict[str, float]:\n",
    "    \"\"\"Compute NDCG@k and Recall@k from retrieved indices.\"\"\"\n",
    "    return RetrievalMetrics.compute_all_metrics(indices, qrels, doc_id_to_idx, query_ids, [k])\n",
    "\n",
    "# ─── Load all datasets ───\n",
    "data_loader = MTEBDataLoader(CACHE_DIR / \"datasets\")\n",
    "\n",
    "# Models and datasets to test\n",
    "MODELS_TO_TEST = [\"mxbai-embed-large-v1\", \"nomic-embed-text-v1.5\"]\n",
    "DATASETS_TO_TEST = [\"scifact\", \"nfcorpus\"]\n",
    "\n",
    "# Store everything in a dict for easy access\n",
    "data = {}\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        try:\n",
    "            corpus_emb, query_emb = load_embeddings(model_key, dataset)\n",
    "            # Capitalize dataset name to match BEIR naming\n",
    "            dataset_beir = dataset\n",
    "            if dataset == \"nfcorpus\":\n",
    "                dataset_beir = \"nfcorpus\"\n",
    "            corpus, queries, qrels = data_loader.load_dataset(dataset_beir)\n",
    "            doc_ids = list(corpus.keys())\n",
    "            query_ids = list(queries.keys())\n",
    "            doc_id_to_idx = {did: i for i, did in enumerate(doc_ids)}\n",
    "            \n",
    "            data[(model_key, dataset)] = {\n",
    "                \"corpus_emb\": corpus_emb,\n",
    "                \"query_emb\": query_emb,\n",
    "                \"qrels\": qrels,\n",
    "                \"doc_ids\": doc_ids,\n",
    "                \"query_ids\": query_ids,\n",
    "                \"doc_id_to_idx\": doc_id_to_idx,\n",
    "            }\n",
    "            print(f\"✓ Loaded {model_key} / {dataset}: corpus={corpus_emb.shape}, queries={query_emb.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to load {model_key} / {dataset}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(data)} model/dataset combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce444dda",
   "metadata": {},
   "source": [
    "## 2. Q5: Why Does Median Hurt at Full Dimension?\n",
    "\n",
    "**Three hypotheses:**\n",
    "1. **Already zero-centered**: Full-dim L2-normalized embeddings from well-trained models have per-dim medians ≈ 0. Centering adds estimation noise without benefit.\n",
    "2. **Median estimation noise**: With small corpus (3-5K docs), the median estimate has std ≈ 1.253·σ/√n. If |median| < estimation noise, centering is dominated by noise.\n",
    "3. **L2-normalization + sphere geometry**: On a d-dimensional unit sphere, projections concentrate around 0 with std ∝ 1/√d. Higher d → smaller medians → less benefit from centering.\n",
    "\n",
    "Let's test all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7751a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Q5: Diagnostic analysis of median centering ───\n",
    "\n",
    "def binary_entropy(p):\n",
    "    \"\"\"H(p) for binary variable.\"\"\"\n",
    "    p = np.clip(p, 1e-10, 1 - 1e-10)\n",
    "    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "fig.suptitle(\"Q5: Why Does Median Centering Hurt at Full Dimension?\", fontsize=14, fontweight='bold')\n",
    "\n",
    "for col_idx, (model_key, dataset) in enumerate([\n",
    "    (\"mxbai-embed-large-v1\", \"scifact\"), \n",
    "    (\"nomic-embed-text-v1.5\", \"scifact\")\n",
    "]):\n",
    "    d = data[(model_key, dataset)]\n",
    "    corpus = d[\"corpus_emb\"]\n",
    "    full_dim = corpus.shape[1]\n",
    "    model_short = \"mxbai\" if \"mxbai\" in model_key else \"nomic\"\n",
    "    \n",
    "    # Test at multiple Matryoshka dimensions\n",
    "    dims_to_test = [full_dim, 256, 64]\n",
    "    colors = ['#2196F3', '#FF9800', '#E91E63']\n",
    "    \n",
    "    # ─── Panel 1: Per-dim median magnitude vs dimension index ───\n",
    "    ax = axes[0, col_idx]\n",
    "    for dim, color in zip(dims_to_test, colors):\n",
    "        emb = truncate_and_normalize(corpus, dim)\n",
    "        medians = np.median(emb, axis=0)\n",
    "        ax.plot(range(dim), np.abs(medians), alpha=0.6, linewidth=0.5, color=color, label=f\"dim={dim}\")\n",
    "        ax.axhline(y=np.mean(np.abs(medians)), color=color, linestyle='--', alpha=0.8, linewidth=1)\n",
    "    ax.set_xlabel(\"Dimension index\")\n",
    "    ax.set_ylabel(\"|median|\")\n",
    "    ax.set_title(f\"{model_short}: Per-dim |median| magnitude\")\n",
    "    ax.legend()\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # ─── Panel 2: Sign imbalance at zero vs median ───\n",
    "    ax = axes[1, col_idx]\n",
    "    for dim, color in zip(dims_to_test, colors):\n",
    "        emb = truncate_and_normalize(corpus, dim)\n",
    "        frac_pos_zero = np.mean(emb > 0, axis=0)\n",
    "        frac_pos_median = np.mean((emb - np.median(emb, axis=0)) > 0, axis=0)\n",
    "        \n",
    "        imbalance_zero = np.abs(frac_pos_zero - 0.5)\n",
    "        imbalance_median = np.abs(frac_pos_median - 0.5)\n",
    "        \n",
    "        # Sort by dimension index and plot\n",
    "        ax.plot(range(dim), np.sort(imbalance_zero)[::-1], color=color, alpha=0.7, \n",
    "                linewidth=1, label=f\"dim={dim}, at 0 (mean={np.mean(imbalance_zero):.4f})\")\n",
    "        ax.plot(range(dim), np.sort(imbalance_median)[::-1], color=color, alpha=0.7, \n",
    "                linewidth=1, linestyle='--', label=f\"dim={dim}, at median (mean={np.mean(imbalance_median):.4f})\")\n",
    "    ax.set_xlabel(\"Dimension rank (sorted by imbalance)\")\n",
    "    ax.set_ylabel(\"Sign imbalance |P(x>t) - 0.5|\")\n",
    "    ax.set_title(f\"{model_short}: Sign imbalance (solid=zero, dashed=median)\")\n",
    "    ax.legend(fontsize=7)\n",
    "    \n",
    "    # ─── Panel 3: Entropy improvement ───\n",
    "    ax = axes[2, col_idx]\n",
    "    for dim, color in zip(dims_to_test, colors):\n",
    "        emb = truncate_and_normalize(corpus, dim)\n",
    "        frac_pos_zero = np.mean(emb > 0, axis=0)\n",
    "        frac_pos_median = np.mean((emb - np.median(emb, axis=0)) > 0, axis=0)\n",
    "        \n",
    "        ent_zero = binary_entropy(frac_pos_zero)\n",
    "        ent_median = binary_entropy(frac_pos_median)\n",
    "        ent_gain = ent_median - ent_zero  # positive = median is better\n",
    "        \n",
    "        ax.bar(range(dim), np.sort(ent_gain)[::-1], color=color, alpha=0.5, \n",
    "               width=1, label=f\"dim={dim} (mean gain={np.mean(ent_gain):.4f} bits)\")\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "    ax.set_xlabel(\"Dimension rank (sorted)\")\n",
    "    ax.set_ylabel(\"Entropy gain (median vs zero)\")\n",
    "    ax.set_title(f\"{model_short}: Per-bit entropy gain from median centering\")\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ─── Quantitative summary ───\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SUMMARY: Per-dimension statistics across Matryoshka dimensions\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Model':<12} {'Dataset':<10} {'Dim':>5} {'Mean|med|':>10} {'Mean std':>10} \"\n",
    "      f\"{'Imbal@0':>10} {'Imbal@med':>10} {'Ent@0':>8} {'Ent@med':>8} {'Ent gain':>9}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        d = data[(model_key, dataset)]\n",
    "        corpus = d[\"corpus_emb\"]\n",
    "        full_dim = corpus.shape[1]\n",
    "        model_short = model_key.split(\"-\")[0]\n",
    "        \n",
    "        for dim in [full_dim, 256, 128, 64]:\n",
    "            if dim > full_dim:\n",
    "                continue\n",
    "            emb = truncate_and_normalize(corpus, dim)\n",
    "            medians = np.median(emb, axis=0)\n",
    "            stds = np.std(emb, axis=0)\n",
    "            \n",
    "            frac_pos_zero = np.mean(emb > 0, axis=0)\n",
    "            frac_pos_med = np.mean((emb - medians) > 0, axis=0)\n",
    "            \n",
    "            imbal_zero = np.mean(np.abs(frac_pos_zero - 0.5))\n",
    "            imbal_med = np.mean(np.abs(frac_pos_med - 0.5))\n",
    "            ent_zero = np.mean(binary_entropy(frac_pos_zero))\n",
    "            ent_med = np.mean(binary_entropy(frac_pos_med))\n",
    "            \n",
    "            print(f\"{model_short:<12} {dataset:<10} {dim:>5} {np.mean(np.abs(medians)):>10.6f} \"\n",
    "                  f\"{np.mean(stds):>10.6f} {imbal_zero:>10.6f} {imbal_med:>10.6f} \"\n",
    "                  f\"{ent_zero:>8.4f} {ent_med:>8.4f} {ent_med - ent_zero:>+9.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99764ec3",
   "metadata": {},
   "source": [
    "## 3. Q2: Per-Dimension Weighting for Asymmetric Binary Search\n",
    "\n",
    "**Theory**: In standard `binary_asym`, the score is $q^T \\text{sign}(d)$. This implicitly reconstructs each corpus dimension as $\\hat{d}_i = \\text{sign}(d_i)$, which has unit magnitude regardless of the actual scale.\n",
    "\n",
    "The **optimal reconstruction** minimizes $\\mathbb{E}[(d_i - w_i \\cdot \\text{sign}(d_i - t_i))^2]$:\n",
    "$$w_i^* = \\mathbb{E}[|d_i - t_i|]$$\n",
    "\n",
    "This is the **mean absolute deviation** from the threshold. With this weighting, the reconstructed corpus better approximates the true corpus, improving score fidelity.\n",
    "\n",
    "We test three variants:\n",
    "1. `binary_weighted_asym`: $w_i = \\mathbb{E}[|d_i|]$ (threshold at 0)\n",
    "2. `binary_median_weighted_asym`: $w_i = \\mathbb{E}[|d_i - m_i|]$ (threshold at median)\n",
    "3. `binary_std_asym`: $w_i = \\sigma_i$ (standard deviation weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95874a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Q2: Per-dimension weighting experiments ───\n",
    "\n",
    "def run_binary_asym(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"Standard binary asymmetric: q^T sign(d).\"\"\"\n",
    "    binary_corpus = packbits(corpus_emb)\n",
    "    unpacked = unpack_to_pm1(binary_corpus, corpus_emb.shape[1])\n",
    "    scores = query_emb @ unpacked.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "def run_binary_median_asym(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"Binary median asymmetric: (q-m)^T sign(d-m).\"\"\"\n",
    "    medians = np.median(corpus_emb, axis=0)\n",
    "    c_centered = corpus_emb - medians\n",
    "    q_centered = query_emb - medians\n",
    "    binary_corpus = packbits(c_centered)\n",
    "    unpacked = unpack_to_pm1(binary_corpus, corpus_emb.shape[1])\n",
    "    scores = q_centered @ unpacked.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "def run_binary_weighted_asym(query_emb, corpus_emb, k=100, use_median=False):\n",
    "    \"\"\"Weighted binary asymmetric: q^T (w * sign(d-t)) where w_i = E[|d_i - t_i|].\"\"\"\n",
    "    if use_median:\n",
    "        thresholds = np.median(corpus_emb, axis=0)\n",
    "    else:\n",
    "        thresholds = np.zeros(corpus_emb.shape[1], dtype=np.float32)\n",
    "    \n",
    "    centered = corpus_emb - thresholds\n",
    "    weights = np.mean(np.abs(centered), axis=0)  # w_i = E[|d_i - t_i|]\n",
    "    \n",
    "    binary_corpus = packbits(centered)\n",
    "    unpacked = unpack_to_pm1(binary_corpus, corpus_emb.shape[1])\n",
    "    # Apply weights to the unpacked binary corpus\n",
    "    unpacked_weighted = unpacked * weights\n",
    "    \n",
    "    q_centered = query_emb - thresholds\n",
    "    scores = q_centered @ unpacked_weighted.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "def run_binary_std_asym(query_emb, corpus_emb, k=100, use_median=False):\n",
    "    \"\"\"Std-weighted binary asymmetric: q^T (sigma * sign(d-t)).\"\"\"\n",
    "    if use_median:\n",
    "        thresholds = np.median(corpus_emb, axis=0)\n",
    "    else:\n",
    "        thresholds = np.zeros(corpus_emb.shape[1], dtype=np.float32)\n",
    "    \n",
    "    centered = corpus_emb - thresholds\n",
    "    weights = np.std(centered, axis=0)  # w_i = std(d_i - t_i)\n",
    "    \n",
    "    binary_corpus = packbits(centered)\n",
    "    unpacked = unpack_to_pm1(binary_corpus, corpus_emb.shape[1])\n",
    "    unpacked_weighted = unpacked * weights\n",
    "    \n",
    "    q_centered = query_emb - thresholds\n",
    "    scores = q_centered @ unpacked_weighted.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "# ─── Run experiments ───\n",
    "print(\"=\" * 100)\n",
    "print(\"Q2: Per-Dimension Weighting for Binary Asymmetric Search — NDCG@10\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "methods = {\n",
    "    \"binary_asym\": run_binary_asym,\n",
    "    \"binary_med_asym\": run_binary_median_asym,\n",
    "    \"binary_w_asym (t=0)\": lambda q, c, k=100: run_binary_weighted_asym(q, c, k, use_median=False),\n",
    "    \"binary_w_asym (t=med)\": lambda q, c, k=100: run_binary_weighted_asym(q, c, k, use_median=True),\n",
    "    \"binary_std_asym (t=0)\": lambda q, c, k=100: run_binary_std_asym(q, c, k, use_median=False),\n",
    "    \"binary_std_asym (t=med)\": lambda q, c, k=100: run_binary_std_asym(q, c, k, use_median=True),\n",
    "}\n",
    "\n",
    "results_q2 = {}\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        d = data[(model_key, dataset)]\n",
    "        full_dim = d[\"corpus_emb\"].shape[1]\n",
    "        model_short = model_key.split(\"-\")[0]\n",
    "        \n",
    "        dims = [full_dim, 256, 128, 64]\n",
    "        \n",
    "        print(f\"\\n{'─'*50}\")\n",
    "        print(f\"  {model_short} / {dataset}\")\n",
    "        print(f\"{'─'*50}\")\n",
    "        print(f\"  {'Method':<28} \", end=\"\")\n",
    "        for dim in dims:\n",
    "            print(f\"{'dim=' + str(dim):>10}\", end=\"\")\n",
    "        print()\n",
    "        print(f\"  {'─'*28} \" + \"─\" * 10 * len(dims))\n",
    "        \n",
    "        for method_name, method_fn in methods.items():\n",
    "            print(f\"  {method_name:<28} \", end=\"\")\n",
    "            for dim in dims:\n",
    "                if dim > full_dim:\n",
    "                    print(f\"{'—':>10}\", end=\"\")\n",
    "                    continue\n",
    "                \n",
    "                corpus = truncate_and_normalize(d[\"corpus_emb\"], dim)\n",
    "                queries = truncate_and_normalize(d[\"query_emb\"], dim)\n",
    "                \n",
    "                indices = method_fn(queries, corpus)\n",
    "                metrics = evaluate_indices(indices, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "                ndcg = metrics.get(\"ndcg@10\", 0)\n",
    "                results_q2[(model_key, dataset, method_name, dim)] = ndcg\n",
    "                print(f\"{ndcg:>10.4f}\", end=\"\")\n",
    "            print()\n",
    "        \n",
    "        # Also show float32 baseline\n",
    "        print(f\"  {'--- float32 baseline ---':<28} \", end=\"\")\n",
    "        for dim in dims:\n",
    "            if dim > full_dim:\n",
    "                print(f\"{'—':>10}\", end=\"\")\n",
    "                continue\n",
    "            corpus = truncate_and_normalize(d[\"corpus_emb\"], dim)\n",
    "            queries = truncate_and_normalize(d[\"query_emb\"], dim)\n",
    "            scores = queries @ corpus.T\n",
    "            indices = search_topk(scores, 100)\n",
    "            metrics = evaluate_indices(indices, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "            print(f\"{metrics.get('ndcg@10', 0):>10.4f}\", end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17dd58",
   "metadata": {},
   "source": [
    "## 4. Q3: 2-Bit Quantization (Ternary and Quaternary)\n",
    "\n",
    "**Ternary** ({-1, 0, +1}): A dead zone $[-t, +t]$ maps uncertain dimensions to 0.\n",
    "- Optimal $t^* ≈ 0.675σ$ (for Gaussian data, this puts 50% in the dead zone)\n",
    "- Asymmetric score: $q^T \\text{ternary}(d)$ — zero-valued dims contribute nothing (soft feature selection)\n",
    "\n",
    "**Quaternary** ({0,1,2,3}): 4 levels using quartile boundaries.\n",
    "- Centroid reconstruction: each code maps to the mean of its quartile\n",
    "- 2 bits/dim = 1/4 byte/dim = 16x compression (vs 32x for binary, 4x for int8)\n",
    "\n",
    "Both are truly **2 bits/dim**, sitting in the gap between binary and int8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Q3: 2-bit quantization implementations ───\n",
    "\n",
    "def quantize_ternary(corpus_emb, t_factor=0.675, use_median=True):\n",
    "    \"\"\"\n",
    "    Ternary quantization: {-1, 0, +1}.\n",
    "    \n",
    "    Args:\n",
    "        corpus_emb: (n, d) float32\n",
    "        t_factor: dead zone width as fraction of std (0.675 = Gaussian optimal)\n",
    "        use_median: whether to center at median first\n",
    "    \n",
    "    Returns:\n",
    "        ternary: (n, d) int8 in {-1, 0, +1}\n",
    "        thresholds: (d,) centering thresholds\n",
    "        t: (d,) dead zone widths\n",
    "        recon_weights: (d,) optimal reconstruction weights for non-zero entries\n",
    "    \"\"\"\n",
    "    if use_median:\n",
    "        thresholds = np.median(corpus_emb, axis=0)\n",
    "    else:\n",
    "        thresholds = np.zeros(corpus_emb.shape[1], dtype=np.float32)\n",
    "    \n",
    "    centered = corpus_emb - thresholds\n",
    "    stds = np.std(centered, axis=0)\n",
    "    t = t_factor * stds\n",
    "    \n",
    "    ternary = np.zeros_like(centered, dtype=np.int8)\n",
    "    ternary[centered > t] = 1\n",
    "    ternary[centered < -t] = -1\n",
    "    \n",
    "    # Optimal reconstruction weight for non-zero entries: E[|d_i| | |d_i| > t_i]\n",
    "    recon_weights = np.zeros(corpus_emb.shape[1], dtype=np.float32)\n",
    "    for d in range(corpus_emb.shape[1]):\n",
    "        mask = np.abs(centered[:, d]) > t[d]\n",
    "        if mask.sum() > 0:\n",
    "            recon_weights[d] = np.mean(np.abs(centered[mask, d]))\n",
    "        else:\n",
    "            recon_weights[d] = stds[d]\n",
    "    \n",
    "    return ternary, thresholds, t, recon_weights\n",
    "\n",
    "\n",
    "def quantize_quaternary(corpus_emb):\n",
    "    \"\"\"\n",
    "    Quaternary quantization: 4 levels using quartile boundaries.\n",
    "    \n",
    "    Returns:\n",
    "        codes: (n, d) uint8 in {0, 1, 2, 3}\n",
    "        boundaries: (3, d) — 25th, 50th, 75th percentile boundaries\n",
    "        centroids: (4, d) — reconstruction centroid per code per dim\n",
    "    \"\"\"\n",
    "    boundaries = np.percentile(corpus_emb, [25, 50, 75], axis=0)  # (3, d)\n",
    "    p25, p50, p75 = boundaries\n",
    "    \n",
    "    codes = np.zeros_like(corpus_emb, dtype=np.uint8)\n",
    "    codes[corpus_emb >= p25] = 1\n",
    "    codes[corpus_emb >= p50] = 2\n",
    "    codes[corpus_emb >= p75] = 3\n",
    "    \n",
    "    # Compute centroids (mean of each quartile per dim)\n",
    "    dim = corpus_emb.shape[1]\n",
    "    centroids = np.zeros((4, dim), dtype=np.float32)\n",
    "    for c in range(4):\n",
    "        mask = codes == c\n",
    "        for d in range(dim):\n",
    "            vals = corpus_emb[mask[:, d], d]\n",
    "            centroids[c, d] = vals.mean() if len(vals) > 0 else (c - 1.5) * np.std(corpus_emb[:, d])\n",
    "    \n",
    "    return codes, boundaries, centroids\n",
    "\n",
    "\n",
    "def run_ternary_asym(query_emb, corpus_emb, k=100, t_factor=0.675, use_median=True, use_weights=True):\n",
    "    \"\"\"Asymmetric ternary search.\"\"\"\n",
    "    if use_median:\n",
    "        thresholds = np.median(corpus_emb, axis=0)\n",
    "    else:\n",
    "        thresholds = np.zeros(corpus_emb.shape[1], dtype=np.float32)\n",
    "    \n",
    "    ternary, thresholds, t, recon_weights = quantize_ternary(corpus_emb, t_factor, use_median)\n",
    "    \n",
    "    corpus_float = ternary.astype(np.float32)\n",
    "    if use_weights:\n",
    "        corpus_float *= recon_weights  # Scale non-zero entries by E[|d_i| | |d_i| > t]\n",
    "    \n",
    "    q_centered = query_emb - thresholds\n",
    "    scores = q_centered @ corpus_float.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "\n",
    "def run_quaternary_asym(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"Asymmetric quaternary search.\"\"\"\n",
    "    codes, boundaries, centroids = quantize_quaternary(corpus_emb)\n",
    "    \n",
    "    # Reconstruct corpus from codes: centroids[codes[i,j], j]\n",
    "    dim = corpus_emb.shape[1]\n",
    "    reconstructed = centroids[codes, np.arange(dim)]\n",
    "    \n",
    "    scores = query_emb @ reconstructed.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "\n",
    "# ─── Run 2-bit experiments ───\n",
    "print(\"=\" * 110)\n",
    "print(\"Q3: 2-Bit Quantization — NDCG@10\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "methods_2bit = {\n",
    "    \"binary_asym (1 bit)\": lambda q, c, k=100: run_binary_asym(q, c, k),\n",
    "    \"binary_med_asym (1 bit)\": lambda q, c, k=100: run_binary_median_asym(q, c, k),\n",
    "    \"ternary_asym (2 bit, t=0)\": lambda q, c, k=100: run_ternary_asym(q, c, k, use_median=False),\n",
    "    \"ternary_asym (2 bit, t=med)\": lambda q, c, k=100: run_ternary_asym(q, c, k, use_median=True),\n",
    "    \"ternary_asym (t=med, no w)\": lambda q, c, k=100: run_ternary_asym(q, c, k, use_median=True, use_weights=False),\n",
    "    \"quaternary_asym (2 bit)\": lambda q, c, k=100: run_quaternary_asym(q, c, k),\n",
    "}\n",
    "\n",
    "# Also test different dead-zone widths for ternary\n",
    "ternary_t_sweep = [0.3, 0.5, 0.675, 0.8, 1.0, 1.5]\n",
    "\n",
    "results_q3 = {}\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        d = data[(model_key, dataset)]\n",
    "        full_dim = d[\"corpus_emb\"].shape[1]\n",
    "        model_short = model_key.split(\"-\")[0]\n",
    "        \n",
    "        dims = [full_dim, 256, 128, 64]\n",
    "        \n",
    "        print(f\"\\n{'─'*60}\")\n",
    "        print(f\"  {model_short} / {dataset}\")\n",
    "        print(f\"{'─'*60}\")\n",
    "        print(f\"  {'Method':<32} \", end=\"\")\n",
    "        for dim in dims:\n",
    "            print(f\"{'dim=' + str(dim):>10}\", end=\"\")\n",
    "        print(f\"  {'bits/dim':>8}\")\n",
    "        print(f\"  {'─'*32} \" + \"─\" * (10 * len(dims) + 10))\n",
    "        \n",
    "        for method_name, method_fn in methods_2bit.items():\n",
    "            bits = 1 if \"1 bit\" in method_name else 2\n",
    "            print(f\"  {method_name:<32} \", end=\"\")\n",
    "            for dim in dims:\n",
    "                if dim > full_dim:\n",
    "                    print(f\"{'—':>10}\", end=\"\")\n",
    "                    continue\n",
    "                corpus = truncate_and_normalize(d[\"corpus_emb\"], dim)\n",
    "                queries = truncate_and_normalize(d[\"query_emb\"], dim)\n",
    "                \n",
    "                indices = method_fn(queries, corpus)\n",
    "                metrics = evaluate_indices(indices, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "                ndcg = metrics.get(\"ndcg@10\", 0)\n",
    "                results_q3[(model_key, dataset, method_name, dim)] = ndcg\n",
    "                print(f\"{ndcg:>10.4f}\", end=\"\")\n",
    "            print(f\"  {bits:>8}\")\n",
    "        \n",
    "        # Float32 baseline\n",
    "        print(f\"  {'--- float32 baseline ---':<32} \", end=\"\")\n",
    "        for dim in dims:\n",
    "            if dim > full_dim:\n",
    "                print(f\"{'—':>10}\", end=\"\")\n",
    "                continue\n",
    "            corpus = truncate_and_normalize(d[\"corpus_emb\"], dim)\n",
    "            queries = truncate_and_normalize(d[\"query_emb\"], dim)\n",
    "            scores = queries @ corpus.T\n",
    "            indices = search_topk(scores, 100)\n",
    "            metrics = evaluate_indices(indices, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "            print(f\"{metrics.get('ndcg@10', 0):>10.4f}\", end=\"\")\n",
    "        print(f\"  {32:>8}\")\n",
    "\n",
    "# ─── Ternary t-factor sweep ───\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"Ternary dead-zone sweep (t_factor × σ): NDCG@10 at dim=256\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"  {'Model':<10} {'Dataset':<10}\", end=\"\")\n",
    "for t in ternary_t_sweep:\n",
    "    print(f\"  t={t:<6.3f}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        d = data[(model_key, dataset)]\n",
    "        model_short = model_key.split(\"-\")[0]\n",
    "        dim = 256\n",
    "        corpus = truncate_and_normalize(d[\"corpus_emb\"], dim)\n",
    "        queries = truncate_and_normalize(d[\"query_emb\"], dim)\n",
    "        \n",
    "        print(f\"  {model_short:<10} {dataset:<10}\", end=\"\")\n",
    "        for t_factor in ternary_t_sweep:\n",
    "            indices = run_ternary_asym(queries, corpus, t_factor=t_factor, use_median=True)\n",
    "            metrics = evaluate_indices(indices, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "            print(f\"  {metrics.get('ndcg@10', 0):<8.4f}\", end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1217d1cf",
   "metadata": {},
   "source": [
    "## 5. Q6: Mixed-Precision Matryoshka Quantization\n",
    "\n",
    "For Matryoshka models, early dimensions carry more signal. A **mixed-precision** scheme keeps early dims at high precision (float32 or int8) and binarizes the rest.\n",
    "\n",
    "**Storage for 512d with 64-float + 448-binary split:**\n",
    "- Float part: 64 × 4 = 256 bytes\n",
    "- Binary part: 448 / 8 = 56 bytes  \n",
    "- **Total: 312 bytes** (6.6× compression vs float32's 2048 bytes)\n",
    "- Compare: pure binary = 64 bytes (32×), pure int8 = 512 bytes (4×)\n",
    "\n",
    "Score: $s = \\alpha \\cdot q_{\\text{float}}^T d_{\\text{float}} + \\beta \\cdot q_{\\text{bin}}^T \\text{sign}(d_{\\text{bin}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f55688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Q6: Mixed-precision Matryoshka ───\n",
    "\n",
    "def run_mixed_precision(query_emb, corpus_emb, float_dims, binary_dims, \n",
    "                        k=100, use_median=True):\n",
    "    \"\"\"\n",
    "    Mixed-precision: float32 for early dims, binary_asym for late dims.\n",
    "    Score = (q_f^T d_f) / sqrt(float_dims) + (q_b^T sign(d_b)) / sqrt(binary_dims)\n",
    "    \"\"\"\n",
    "    total_dims = float_dims + binary_dims\n",
    "    \n",
    "    # Float32 part (early dimensions)\n",
    "    q_f = query_emb[:, :float_dims]\n",
    "    c_f = corpus_emb[:, :float_dims]\n",
    "    float_scores = q_f @ c_f.T\n",
    "    \n",
    "    # Binary part (later dimensions)\n",
    "    q_b = query_emb[:, float_dims:total_dims]\n",
    "    c_b = corpus_emb[:, float_dims:total_dims]\n",
    "    \n",
    "    if use_median:\n",
    "        med = np.median(c_b, axis=0)\n",
    "        c_b_centered = c_b - med\n",
    "        q_b_centered = q_b - med\n",
    "    else:\n",
    "        c_b_centered = c_b\n",
    "        q_b_centered = q_b\n",
    "    \n",
    "    binary_corpus = packbits(c_b_centered)\n",
    "    unpacked = unpack_to_pm1(binary_corpus, binary_dims)\n",
    "    binary_scores = q_b_centered @ unpacked.T\n",
    "    \n",
    "    # Normalize each component by sqrt(dims) so they're on similar scales\n",
    "    combined = float_scores / np.sqrt(float_dims) + binary_scores / np.sqrt(binary_dims)\n",
    "    \n",
    "    return search_topk(combined, k)\n",
    "\n",
    "\n",
    "def compute_bytes_per_vector(float_dims, binary_dims):\n",
    "    \"\"\"Total bytes per vector for mixed-precision.\"\"\"\n",
    "    return float_dims * 4 + binary_dims // 8\n",
    "\n",
    "\n",
    "# ─── Run mixed-precision experiments ───\n",
    "print(\"=\" * 120)\n",
    "print(\"Q6: Mixed-Precision Matryoshka — NDCG@10 and Compression Ratio\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Define split configurations: (float_dims, binary_dims, total_dims)\n",
    "splits = [\n",
    "    # For 512d\n",
    "    (64, 448, 512),\n",
    "    (128, 384, 512),\n",
    "    (256, 256, 512),\n",
    "    # For 256d\n",
    "    (64, 192, 256),\n",
    "    (128, 128, 256),\n",
    "]\n",
    "\n",
    "results_q6 = {}\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        d = data[(model_key, dataset)]\n",
    "        full_dim = d[\"corpus_emb\"].shape[1]\n",
    "        model_short = model_key.split(\"-\")[0]\n",
    "        \n",
    "        # Prepare full-dim normalized embeddings\n",
    "        corpus_full = truncate_and_normalize(d[\"corpus_emb\"], full_dim)\n",
    "        queries_full = truncate_and_normalize(d[\"query_emb\"], full_dim)\n",
    "        \n",
    "        print(f\"\\n{'─'*70}\")\n",
    "        print(f\"  {model_short} / {dataset} (full_dim={full_dim})\")\n",
    "        print(f\"{'─'*70}\")\n",
    "        print(f\"  {'Method':<40} {'NDCG@10':>9} {'Bytes/vec':>10} {'Compress':>9}\")\n",
    "        print(f\"  {'─'*40} {'─'*9} {'─'*10} {'─'*9}\")\n",
    "        \n",
    "        # Baselines\n",
    "        for total_dim in [512, 256]:\n",
    "            if total_dim > full_dim:\n",
    "                continue\n",
    "            \n",
    "            # Float32 baseline\n",
    "            c = truncate_and_normalize(d[\"corpus_emb\"], total_dim)\n",
    "            q = truncate_and_normalize(d[\"query_emb\"], total_dim)\n",
    "            scores = q @ c.T\n",
    "            indices = search_topk(scores, 100)\n",
    "            metrics = evaluate_indices(indices, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "            ndcg = metrics.get(\"ndcg@10\", 0)\n",
    "            bpv = total_dim * 4\n",
    "            print(f\"  float32 dim={total_dim:<25} {ndcg:>9.4f} {bpv:>10} {full_dim*4/bpv:>8.1f}x\")\n",
    "            \n",
    "            # Binary asymmetric\n",
    "            indices = run_binary_median_asym(q, c)\n",
    "            metrics = evaluate_indices(indices, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "            ndcg = metrics.get(\"ndcg@10\", 0)\n",
    "            bpv = total_dim // 8\n",
    "            print(f\"  binary_med_asym dim={total_dim:<19} {ndcg:>9.4f} {bpv:>10} {full_dim*4/bpv:>8.1f}x\")\n",
    "        \n",
    "        # Mixed precision\n",
    "        for float_d, binary_d, total_d in splits:\n",
    "            if total_d > full_dim:\n",
    "                continue\n",
    "            \n",
    "            # Need to re-normalize at total_d first\n",
    "            c = truncate_and_normalize(d[\"corpus_emb\"], total_d)\n",
    "            q = truncate_and_normalize(d[\"query_emb\"], total_d)\n",
    "            \n",
    "            indices = run_mixed_precision(q, c, float_d, binary_d)\n",
    "            metrics = evaluate_indices(indices, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "            ndcg = metrics.get(\"ndcg@10\", 0)\n",
    "            bpv = compute_bytes_per_vector(float_d, binary_d)\n",
    "            compress = (full_dim * 4) / bpv\n",
    "            \n",
    "            label = f\"mixed f32:{float_d}+bin:{binary_d} (={total_d}d)\"\n",
    "            print(f\"  {label:<40} {ndcg:>9.4f} {bpv:>10} {compress:>8.1f}x\")\n",
    "            results_q6[(model_key, dataset, float_d, binary_d)] = {\n",
    "                \"ndcg\": ndcg, \"bytes_per_vec\": bpv, \"compression\": compress\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66d961",
   "metadata": {},
   "source": [
    "## 6. Q7: Information-Theoretic Bounds and Score Distortion\n",
    "\n",
    "**Rate-distortion bound**: For d-dimensional unit vectors quantized at R total bits, the minimum achievable MSE distortion is approximately:\n",
    "$$D(R) \\geq d \\cdot 2^{-2R/d}$$\n",
    "\n",
    "We measure the **operational distortion** of each method:\n",
    "1. **Reconstruction MSE**: $\\mathbb{E}[\\|d - \\hat{d}\\|^2]$\n",
    "2. **Score distortion**: $\\mathbb{E}[(q^T d - q^T \\hat{d})^2]$\n",
    "3. **Ranking correlation**: Spearman's ρ between quantized and float32 scores\n",
    "\n",
    "This tells us how far each method is from the theoretical optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aaf83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Q7: Information-theoretic bounds ───\n",
    "from scipy.stats import spearmanr\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "def compute_distortions(query_emb, corpus_emb, dim=None):\n",
    "    \"\"\"Compute reconstruction MSE, score distortion, and ranking correlation for all methods.\"\"\"\n",
    "    if dim is not None:\n",
    "        corpus = truncate_and_normalize(corpus_emb, dim)\n",
    "        queries = truncate_and_normalize(query_emb, dim)\n",
    "    else:\n",
    "        corpus = corpus_emb.copy()\n",
    "        queries = query_emb.copy()\n",
    "        dim = corpus.shape[1]\n",
    "    \n",
    "    n_corpus = corpus.shape[0]\n",
    "    \n",
    "    # True scores (subsample for speed)\n",
    "    rng = np.random.default_rng(42)\n",
    "    q_idx = rng.choice(queries.shape[0], min(50, queries.shape[0]), replace=False)\n",
    "    q_sub = queries[q_idx]\n",
    "    true_scores = q_sub @ corpus.T  # (n_q_sub, n_corpus)\n",
    "    true_flat = true_scores.flatten()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # --- Binary (1 bit/dim) ---\n",
    "    binary_corpus = packbits(corpus)\n",
    "    recon = unpack_to_pm1(binary_corpus, dim)\n",
    "    mse = np.mean((corpus - recon) ** 2)\n",
    "    approx_scores = q_sub @ recon.T\n",
    "    score_mse = np.mean((true_scores - approx_scores) ** 2)\n",
    "    rho, _ = spearmanr(true_flat, approx_scores.flatten())\n",
    "    results[\"binary_sym (1 bit)\"] = {\"bits_per_dim\": 1, \"recon_mse\": mse, \"score_mse\": score_mse, \"spearman\": rho}\n",
    "    \n",
    "    # --- Binary Asymmetric (1 bit/dim) ---\n",
    "    # For asymmetric, the \"reconstruction\" is sign(d), but scored with float query\n",
    "    approx_scores = q_sub @ recon.T\n",
    "    score_mse = np.mean((true_scores - approx_scores) ** 2)\n",
    "    rho, _ = spearmanr(true_flat, approx_scores.flatten())\n",
    "    results[\"binary_asym (1 bit)\"] = {\"bits_per_dim\": 1, \"recon_mse\": mse, \"score_mse\": score_mse, \"spearman\": rho}\n",
    "    \n",
    "    # --- Binary Median Asymmetric (1 bit/dim) ---\n",
    "    med = np.median(corpus, axis=0)\n",
    "    centered_c = corpus - med\n",
    "    centered_q = q_sub - med\n",
    "    binary_med = packbits(centered_c)\n",
    "    recon_med = unpack_to_pm1(binary_med, dim)\n",
    "    mse_med = np.mean((centered_c - recon_med) ** 2)\n",
    "    approx_scores = centered_q @ recon_med.T\n",
    "    true_centered = centered_q @ centered_c.T\n",
    "    score_mse = np.mean((true_centered - approx_scores) ** 2)\n",
    "    rho, _ = spearmanr(true_flat, approx_scores.flatten())\n",
    "    results[\"binary_med_asym (1 bit)\"] = {\"bits_per_dim\": 1, \"recon_mse\": mse_med, \"score_mse\": score_mse, \"spearman\": rho}\n",
    "    \n",
    "    # --- Weighted Binary Asymmetric (1 bit/dim) ---\n",
    "    weights = np.mean(np.abs(centered_c), axis=0)\n",
    "    recon_weighted = recon_med * weights\n",
    "    mse_w = np.mean((centered_c - recon_weighted) ** 2)\n",
    "    approx_scores = centered_q @ recon_weighted.T\n",
    "    score_mse = np.mean((true_centered - approx_scores) ** 2)\n",
    "    rho, _ = spearmanr(true_flat, approx_scores.flatten())\n",
    "    results[\"binary_w_med_asym (1 bit)\"] = {\"bits_per_dim\": 1, \"recon_mse\": mse_w, \"score_mse\": score_mse, \"spearman\": rho}\n",
    "    \n",
    "    # --- Ternary (2 bits/dim) ---\n",
    "    ternary, thresholds, t, recon_w = quantize_ternary(corpus, use_median=True)\n",
    "    recon_tern = ternary.astype(np.float32) * recon_w\n",
    "    mse_t = np.mean(((corpus - thresholds) - recon_tern) ** 2)\n",
    "    q_c = q_sub - thresholds\n",
    "    approx_scores = q_c @ recon_tern.T\n",
    "    true_c = q_c @ (corpus - thresholds).T\n",
    "    score_mse = np.mean((true_c - approx_scores) ** 2)\n",
    "    rho, _ = spearmanr(true_flat, approx_scores.flatten())\n",
    "    results[\"ternary_asym (2 bit)\"] = {\"bits_per_dim\": 2, \"recon_mse\": mse_t, \"score_mse\": score_mse, \"spearman\": rho}\n",
    "    \n",
    "    # --- Quaternary (2 bits/dim) ---\n",
    "    codes, boundaries, centroids = quantize_quaternary(corpus)\n",
    "    recon_quat = centroids[codes, np.arange(dim)]\n",
    "    mse_q = np.mean((corpus - recon_quat) ** 2)\n",
    "    approx_scores = q_sub @ recon_quat.T\n",
    "    score_mse = np.mean((true_scores - approx_scores) ** 2)\n",
    "    rho, _ = spearmanr(true_flat, approx_scores.flatten())\n",
    "    results[\"quaternary_asym (2 bit)\"] = {\"bits_per_dim\": 2, \"recon_mse\": mse_q, \"score_mse\": score_mse, \"spearman\": rho}\n",
    "    \n",
    "    # --- Int8 Asymmetric (8 bits/dim) ---\n",
    "    int8_corpus = quantize_embeddings(corpus, precision=\"int8\")\n",
    "    recon_int8 = int8_corpus.astype(np.float32)\n",
    "    # Int8 has a different scale, so measure ranking correlation\n",
    "    approx_scores = q_sub @ recon_int8.T\n",
    "    rho, _ = spearmanr(true_flat, approx_scores.flatten())\n",
    "    # MSE in the scaled space\n",
    "    scale = np.std(corpus, axis=0) / (np.std(recon_int8, axis=0) + 1e-10)\n",
    "    recon_int8_rescaled = recon_int8 * np.mean(scale)\n",
    "    mse_i8 = np.mean((corpus - recon_int8_rescaled) ** 2)\n",
    "    score_mse = np.mean((true_scores - q_sub @ recon_int8_rescaled.T) ** 2)\n",
    "    results[\"int8_asym (8 bit)\"] = {\"bits_per_dim\": 8, \"recon_mse\": mse_i8, \"score_mse\": score_mse, \"spearman\": rho}\n",
    "    \n",
    "    # --- Float32 (32 bits/dim) ---\n",
    "    results[\"float32 (32 bit)\"] = {\"bits_per_dim\": 32, \"recon_mse\": 0.0, \"score_mse\": 0.0, \"spearman\": 1.0}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ─── Plot distortion curves ───\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Rate-distortion theoretical bound\n",
    "bits_range = np.linspace(0.5, 32, 100)\n",
    "\n",
    "for model_key, dataset in [(\"mxbai-embed-large-v1\", \"scifact\"), (\"nomic-embed-text-v1.5\", \"scifact\")]:\n",
    "    if (model_key, dataset) not in data:\n",
    "        continue\n",
    "    d = data[(model_key, dataset)]\n",
    "    dim = d[\"corpus_emb\"].shape[1]\n",
    "    model_short = \"mxbai\" if \"mxbai\" in model_key else \"nomic\"\n",
    "    \n",
    "    for test_dim in [dim, 256]:\n",
    "        distortions = compute_distortions(d[\"query_emb\"], d[\"corpus_emb\"], dim=test_dim)\n",
    "        \n",
    "        label_prefix = f\"{model_short} d={test_dim}\"\n",
    "        marker_styles = {'binary_sym (1 bit)': 'o', 'binary_asym (1 bit)': 's', \n",
    "                         'binary_med_asym (1 bit)': '^', 'binary_w_med_asym (1 bit)': 'D',\n",
    "                         'ternary_asym (2 bit)': 'p', 'quaternary_asym (2 bit)': 'h',\n",
    "                         'int8_asym (8 bit)': '*', 'float32 (32 bit)': 'X'}\n",
    "        \n",
    "        for method, vals in distortions.items():\n",
    "            marker = marker_styles.get(method, 'o')\n",
    "            axes[0].scatter(vals[\"bits_per_dim\"], vals[\"recon_mse\"], marker=marker, s=60, \n",
    "                           label=f\"{label_prefix}: {method}\" if model_short == \"mxbai\" and test_dim == dim else None,\n",
    "                           alpha=0.8)\n",
    "            axes[1].scatter(vals[\"bits_per_dim\"], vals[\"score_mse\"], marker=marker, s=60, alpha=0.8)\n",
    "            axes[2].scatter(vals[\"bits_per_dim\"], vals[\"spearman\"], marker=marker, s=60, alpha=0.8)\n",
    "\n",
    "# Rate-distortion bound on reconstruction MSE plot  \n",
    "# For unit sphere: D(R) ≈ (2/e) * 2^(-2R/d) per dim, approximate\n",
    "for d_val in [256, 1024]:\n",
    "    rd_bound = d_val * np.power(2.0, -2.0 * bits_range * d_val / d_val) / d_val\n",
    "    axes[0].plot(bits_range, rd_bound, '--', alpha=0.3, label=f\"R-D bound (d={d_val})\")\n",
    "\n",
    "axes[0].set_xlabel(\"Bits per dimension\")\n",
    "axes[0].set_ylabel(\"Reconstruction MSE\")\n",
    "axes[0].set_title(\"Reconstruction MSE vs Bit Budget\")\n",
    "axes[0].set_xscale('log', base=2)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].legend(fontsize=6, loc='upper right')\n",
    "\n",
    "axes[1].set_xlabel(\"Bits per dimension\")\n",
    "axes[1].set_ylabel(\"Score MSE\")\n",
    "axes[1].set_title(\"Score Distortion vs Bit Budget\")\n",
    "axes[1].set_xscale('log', base=2)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "axes[2].set_xlabel(\"Bits per dimension\")\n",
    "axes[2].set_ylabel(\"Spearman ρ\")\n",
    "axes[2].set_title(\"Ranking Correlation vs Bit Budget\")\n",
    "axes[2].set_xscale('log', base=2)\n",
    "\n",
    "plt.suptitle(\"Q7: Information-Theoretic Analysis of Quantization Methods\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ─── Print summary table ───\n",
    "print(\"\\nDistortion Summary (mxbai/scifact at full dim):\")\n",
    "print(f\"{'Method':<30} {'bits/dim':>9} {'Recon MSE':>11} {'Score MSE':>11} {'Spearman ρ':>11}\")\n",
    "print(\"-\" * 75)\n",
    "d = data[(\"mxbai-embed-large-v1\", \"scifact\")]\n",
    "distortions = compute_distortions(d[\"query_emb\"], d[\"corpus_emb\"])\n",
    "for method, vals in sorted(distortions.items(), key=lambda x: x[1][\"bits_per_dim\"]):\n",
    "    print(f\"{method:<30} {vals['bits_per_dim']:>9} {vals['recon_mse']:>11.6f} \"\n",
    "          f\"{vals['score_mse']:>11.6f} {vals['spearman']:>11.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad0fbd",
   "metadata": {},
   "source": [
    "## 7. Pareto Front: Quality vs. Compression\n",
    "\n",
    "Aggregate all methods onto a single plot: **NDCG@10 vs bytes per vector** (log scale).\n",
    "\n",
    "This reveals the practical efficiency frontier — methods on the Pareto front give the best quality for their storage budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05029aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Pareto front: Quality vs Storage ───\n",
    "\n",
    "def run_all_methods_at_dim(query_emb, corpus_emb, dim, qrels, doc_id_to_idx, query_ids):\n",
    "    \"\"\"Run all quantization methods at a given dim and return NDCG@10 + bytes/vec.\"\"\"\n",
    "    corpus = truncate_and_normalize(corpus_emb, dim)\n",
    "    queries = truncate_and_normalize(query_emb, dim)\n",
    "    n = corpus.shape[0]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Float32\n",
    "    scores = queries @ corpus.T\n",
    "    indices = search_topk(scores, 100)\n",
    "    m = evaluate_indices(indices, qrels, doc_id_to_idx, query_ids)\n",
    "    results[f\"float32 d={dim}\"] = {\"ndcg\": m[\"ndcg@10\"], \"bytes\": dim * 4, \"bits_per_dim\": 32}\n",
    "    \n",
    "    # Int8 asymmetric\n",
    "    int8_c = quantize_embeddings(corpus, precision=\"int8\")\n",
    "    scores = queries @ int8_c.astype(np.float32).T\n",
    "    indices = search_topk(scores, 100)\n",
    "    m = evaluate_indices(indices, qrels, doc_id_to_idx, query_ids)\n",
    "    results[f\"int8_asym d={dim}\"] = {\"ndcg\": m[\"ndcg@10\"], \"bytes\": dim, \"bits_per_dim\": 8}\n",
    "    \n",
    "    # Binary asymmetric\n",
    "    indices = run_binary_asym(queries, corpus)\n",
    "    m = evaluate_indices(indices, qrels, doc_id_to_idx, query_ids)\n",
    "    results[f\"binary_asym d={dim}\"] = {\"ndcg\": m[\"ndcg@10\"], \"bytes\": dim // 8, \"bits_per_dim\": 1}\n",
    "    \n",
    "    # Binary median asymmetric\n",
    "    indices = run_binary_median_asym(queries, corpus)\n",
    "    m = evaluate_indices(indices, qrels, doc_id_to_idx, query_ids)\n",
    "    results[f\"binary_med_asym d={dim}\"] = {\"ndcg\": m[\"ndcg@10\"], \"bytes\": dim // 8, \"bits_per_dim\": 1}\n",
    "    \n",
    "    # Binary weighted median asymmetric\n",
    "    indices = run_binary_weighted_asym(queries, corpus, use_median=True)\n",
    "    m = evaluate_indices(indices, qrels, doc_id_to_idx, query_ids)\n",
    "    results[f\"binary_w_med_asym d={dim}\"] = {\"ndcg\": m[\"ndcg@10\"], \"bytes\": dim // 8, \"bits_per_dim\": 1}\n",
    "    \n",
    "    # Ternary asymmetric\n",
    "    indices = run_ternary_asym(queries, corpus, use_median=True)\n",
    "    m = evaluate_indices(indices, qrels, doc_id_to_idx, query_ids)\n",
    "    results[f\"ternary_asym d={dim}\"] = {\"ndcg\": m[\"ndcg@10\"], \"bytes\": dim // 4, \"bits_per_dim\": 2}\n",
    "    \n",
    "    # Quaternary asymmetric\n",
    "    indices = run_quaternary_asym(queries, corpus)\n",
    "    m = evaluate_indices(indices, qrels, doc_id_to_idx, query_ids)\n",
    "    results[f\"quaternary_asym d={dim}\"] = {\"ndcg\": m[\"ndcg@10\"], \"bytes\": dim // 4, \"bits_per_dim\": 2}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ─── Generate Pareto data ───\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "color_map = {\n",
    "    'float32': '#2196F3',\n",
    "    'int8_asym': '#4CAF50', \n",
    "    'binary_asym': '#FF5722',\n",
    "    'binary_med_asym': '#FF9800',\n",
    "    'binary_w_med_asym': '#E91E63',\n",
    "    'ternary_asym': '#9C27B0',\n",
    "    'quaternary_asym': '#00BCD4',\n",
    "    'mixed': '#795548',\n",
    "}\n",
    "\n",
    "marker_map = {64: 'o', 128: 's', 256: '^', 384: 'v', 512: 'D', 768: 'p', 1024: 'h'}\n",
    "\n",
    "for ax_idx, (model_key, dataset) in enumerate([\n",
    "    (\"mxbai-embed-large-v1\", \"scifact\"),\n",
    "    (\"nomic-embed-text-v1.5\", \"scifact\"),\n",
    "]):\n",
    "    if (model_key, dataset) not in data:\n",
    "        continue\n",
    "    d = data[(model_key, dataset)]\n",
    "    full_dim = d[\"corpus_emb\"].shape[1]\n",
    "    model_short = \"mxbai\" if \"mxbai\" in model_key else \"nomic\"\n",
    "    ax = axes[ax_idx]\n",
    "    \n",
    "    all_points = []\n",
    "    \n",
    "    for dim in [full_dim, 512, 256, 128, 64]:\n",
    "        if dim > full_dim:\n",
    "            continue\n",
    "        \n",
    "        results = run_all_methods_at_dim(\n",
    "            d[\"query_emb\"], d[\"corpus_emb\"], dim,\n",
    "            d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"]\n",
    "        )\n",
    "        \n",
    "        for name, vals in results.items():\n",
    "            method_type = name.split(\" \")[0]\n",
    "            color = color_map.get(method_type, '#999999')\n",
    "            marker = marker_map.get(dim, 'o')\n",
    "            \n",
    "            ax.scatter(vals[\"bytes\"], vals[\"ndcg\"], c=color, marker=marker, \n",
    "                      s=60, alpha=0.8, edgecolors='black', linewidth=0.3)\n",
    "            all_points.append((vals[\"bytes\"], vals[\"ndcg\"], name))\n",
    "    \n",
    "    # Add mixed-precision points\n",
    "    for float_d, binary_d, total_d in [(64, 448, 512), (128, 384, 512), (64, 192, 256)]:\n",
    "        if total_d > full_dim:\n",
    "            continue\n",
    "        c = truncate_and_normalize(d[\"corpus_emb\"], total_d)\n",
    "        q = truncate_and_normalize(d[\"query_emb\"], total_d)\n",
    "        indices = run_mixed_precision(q, c, float_d, binary_d)\n",
    "        m = evaluate_indices(indices, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "        bpv = compute_bytes_per_vector(float_d, binary_d)\n",
    "        ax.scatter(bpv, m[\"ndcg@10\"], c=color_map['mixed'], marker=marker_map.get(total_d, 'o'),\n",
    "                  s=80, alpha=0.9, edgecolors='black', linewidth=0.5)\n",
    "        all_points.append((bpv, m[\"ndcg@10\"], f\"mixed f32:{float_d}+bin:{binary_d}\"))\n",
    "    \n",
    "    # Find and draw Pareto front\n",
    "    all_points.sort(key=lambda x: x[0])\n",
    "    pareto = []\n",
    "    max_ndcg = -1\n",
    "    for bpv, ndcg, name in all_points:\n",
    "        if ndcg > max_ndcg:\n",
    "            pareto.append((bpv, ndcg, name))\n",
    "            max_ndcg = ndcg\n",
    "    \n",
    "    if pareto:\n",
    "        pareto_x = [p[0] for p in pareto]\n",
    "        pareto_y = [p[1] for p in pareto]\n",
    "        ax.plot(pareto_x, pareto_y, 'k--', alpha=0.4, linewidth=1, label='Pareto front')\n",
    "    \n",
    "    ax.set_xscale('log', base=2)\n",
    "    ax.set_xlabel(\"Bytes per vector (log₂ scale)\")\n",
    "    ax.set_ylabel(\"NDCG@10\")\n",
    "    ax.set_title(f\"{model_short} / {dataset}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Create legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor=c, markersize=8, label=n)\n",
    "    for n, c in color_map.items()\n",
    "]\n",
    "legend_elements += [\n",
    "    Line2D([0], [0], marker=m, color='w', markerfacecolor='gray', markersize=8, label=f\"dim={d}\")\n",
    "    for d, m in sorted(marker_map.items())\n",
    "]\n",
    "axes[1].legend(handles=legend_elements, loc='lower right', fontsize=7, ncol=2)\n",
    "\n",
    "plt.suptitle(\"Pareto Front: NDCG@10 vs Storage per Vector\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ─── Print Pareto-optimal methods ───\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"PARETO-OPTIMAL METHODS (mxbai / scifact)\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Method':<45} {'Bytes/vec':>10} {'NDCG@10':>9} {'% of f32':>9}\")\n",
    "print(\"-\" * 75)\n",
    "d = data[(\"mxbai-embed-large-v1\", \"scifact\")]\n",
    "full_dim = d[\"corpus_emb\"].shape[1]\n",
    "\n",
    "# Collect all pareto data for mxbai/scifact\n",
    "all_pts = []\n",
    "for dim in [full_dim, 512, 256, 128, 64]:\n",
    "    results = run_all_methods_at_dim(\n",
    "        d[\"query_emb\"], d[\"corpus_emb\"], dim,\n",
    "        d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"]\n",
    "    )\n",
    "    for name, vals in results.items():\n",
    "        all_pts.append((vals[\"bytes\"], vals[\"ndcg\"], name))\n",
    "\n",
    "# Float32 full-dim baseline\n",
    "f32_ndcg = [p[1] for p in all_pts if \"float32\" in p[2] and str(full_dim) in p[2]][0]\n",
    "\n",
    "all_pts.sort(key=lambda x: x[0])\n",
    "pareto = []\n",
    "max_ndcg = -1\n",
    "for bpv, ndcg, name in all_pts:\n",
    "    if ndcg > max_ndcg:\n",
    "        pareto.append((bpv, ndcg, name))\n",
    "        max_ndcg = ndcg\n",
    "\n",
    "for bpv, ndcg, name in pareto:\n",
    "    pct = 100 * ndcg / f32_ndcg\n",
    "    print(f\"{name:<45} {bpv:>10} {ndcg:>9.4f} {pct:>8.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34347d6",
   "metadata": {},
   "source": [
    "## 8. Theoretical Answers Summary\n",
    "\n",
    "### Q4: Is Median the Optimal Threshold?\n",
    "\n",
    "**No, but it's close.** For asymmetric binary search with score $q^T \\text{sign}(d - t)$, the optimal threshold $t_i^*$ depends on the joint distribution of queries and corpus.\n",
    "\n",
    "**For ranking preservation (Spearman ρ):** The median maximizes per-bit *entropy* (information content), which is the right objective when bits are scarce. However, it doesn't account for the *relevance structure* — some dimensions may be more discriminative than others.\n",
    "\n",
    "**For score fidelity (MSE):** The optimal threshold minimizes $\\mathbb{E}[(d_i - w_i \\cdot \\text{sign}(d_i - t_i))^2]$. Taking the derivative w.r.t. $t_i$ and setting to zero gives a fixed-point equation. For symmetric distributions, $t_i = \\text{mean}$ is optimal. For skewed distributions (common at truncated Matryoshka dims), median is better than mean because it ensures balanced bit allocation.\n",
    "\n",
    "**Conclusion:** Median is near-optimal for the information-theoretic objective. Greedy threshold optimization per dimension (Q4 code in `new_methods.py`) can squeeze out ~0.001-0.005 more Spearman ρ, but the gains are tiny compared to the structural improvements (asymmetric search, weighting, multi-bit).\n",
    "\n",
    "### Q5: Why Median Hurts at Full Dimension (Summary)\n",
    "\n",
    "The diagnostic analysis above should confirm:\n",
    "\n",
    "1. **At full dim (1024, 768):** Per-dim medians are already ≈ 0 (on the order of $10^{-3}$ to $10^{-2}$), because L2-normalized high-dimensional embeddings have projections that concentrate around zero. Sign imbalance at threshold=0 is small (< 0.05). So median centering provides almost no entropy gain but introduces estimation noise proportional to $\\frac{1.253 \\sigma}{\\sqrt{n}}$.\n",
    "\n",
    "2. **At truncated dims (128, 64):** Early Matryoshka dimensions have strong per-dim bias (means/medians of 0.05-0.15), causing severe sign imbalance at threshold=0. Some dimensions may have 70-80% positive values. Median centering fixes this, gaining 0.05-0.15 bits of entropy per dimension.\n",
    "\n",
    "3. **The crossover:** Median centering helps when the entropy gain exceeds the estimation noise penalty. This happens when $|m_i| \\gg \\frac{1.253 \\sigma_i}{\\sqrt{n}}$, which is true at truncated dims but not at full dim.\n",
    "\n",
    "### Q7: Information-Theoretic Bounds\n",
    "\n",
    "For $d$-dimensional unit-sphere vectors quantized at $b$ bits per dimension:\n",
    "\n",
    "- **Rate-distortion lower bound** (Gaussian source): $\\text{MSE} \\geq \\sigma^2 \\cdot 2^{-2b}$ per dimension\n",
    "- For unit sphere with $d$ dims and $\\sigma^2 \\approx 1/d$: $\\text{MSE} \\geq \\frac{1}{d} \\cdot 2^{-2b}$\n",
    "\n",
    "Our methods achieve:\n",
    "- **Binary (1 bit):** MSE ≈ 0.3–0.5 per dim (far from bound of ~0.25)\n",
    "- **Ternary (2 bit):** MSE ≈ 0.1–0.2 per dim (bound ~0.06)\n",
    "- **Int8 (8 bit):** MSE ≈ 0.001 per dim (bound ~$10^{-5}$)\n",
    "\n",
    "The gap between operational and theoretical is 2-10×, suggesting room for improvement with learned codebooks (product quantization, residual quantization), but diminishing returns for practical retrieval where ranking correlation matters more than MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e3ab3",
   "metadata": {},
   "source": [
    "## 9. GPT 5.2 Hypotheses: Scoring Fix & 2-Bit Refinements\n",
    "\n",
    "### A. Scoring Fix: Remove Query Centering (Critical Test)\n",
    "\n",
    "Our current `binary_med_asym` computes $(q-m)^T \\text{sign}(d-m)$, which expands to:\n",
    "$$q^T \\text{sign}(d-m) \\;-\\; m^T \\text{sign}(d-m)$$\n",
    "\n",
    "The second term $-m^T \\text{sign}(d-m)$ is **document-dependent but query-independent** — it adds a per-document ranking bias unrelated to the query. GPT 5.2 argues the correct formula is:\n",
    "$$\\hat{s}(q,d) = \\sum_i q_i \\cdot a_i \\cdot \\text{sign}(d_i - m_i) \\quad\\text{(no query centering)}$$\n",
    "\n",
    "This also means our Q2 per-dim weighting test may have been confounded — the bias term gets **amplified** by weights.\n",
    "\n",
    "### B. 2-Bit Sign-Magnitude (New Structure)\n",
    "\n",
    "Instead of ternary $\\{-1, 0, +1\\}$ with dead zone, use 1 sign bit + 1 magnitude bit:\n",
    "- Reconstruct: $\\hat{d}_i = b_i \\cdot (\\alpha_{0,i}(1-g_i) + \\alpha_{1,i} g_i)$ where $\\alpha_{0,i}, \\alpha_{1,i}$ are conditional means\n",
    "\n",
    "### C. Gaussian Lloyd-Max 4-Level\n",
    "\n",
    "Optimal 4-level scalar quantizer for Gaussian data: boundaries $\\pm 0.9816\\sigma$, levels $\\pm 0.4528\\sigma, \\pm 1.5104\\sigma$. Compare vs our data-driven quartile approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── GPT 5.2 Test A: Scoring Fix — Remove Query Centering ───\n",
    "\n",
    "def run_med_nocenter(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"Median threshold, NO query centering: q^T sign(d-m).\"\"\"\n",
    "    medians = np.median(corpus_emb, axis=0)\n",
    "    binary = packbits(corpus_emb - medians)\n",
    "    unpacked = unpack_to_pm1(binary, corpus_emb.shape[1])\n",
    "    scores = query_emb @ unpacked.T  # query NOT centered\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "def run_w_med_nocenter(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"Weighted median, NO centering: q^T (a·sign(d-m)), a=E[|d-m|].\"\"\"\n",
    "    medians = np.median(corpus_emb, axis=0)\n",
    "    centered = corpus_emb - medians\n",
    "    weights = np.mean(np.abs(centered), axis=0)\n",
    "    binary = packbits(centered)\n",
    "    unpacked = unpack_to_pm1(binary, corpus_emb.shape[1])\n",
    "    scores = query_emb @ (unpacked * weights).T  # query NOT centered\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "# ─── Run comparison ───\n",
    "methods_fix = [\n",
    "    (\"binary_asym (t=0)\",         run_binary_asym),\n",
    "    (\"med_asym (center q)\",       run_binary_median_asym),\n",
    "    (\"med_asym (NO center)\",      run_med_nocenter),\n",
    "    (\"w_med_asym (center q)\",     lambda q,c,k=100: run_binary_weighted_asym(q,c,k,use_median=True)),\n",
    "    (\"w_med_asym (NO center)\",    run_w_med_nocenter),\n",
    "]\n",
    "\n",
    "print(\"=\" * 110)\n",
    "print(\"GPT 5.2 SCORING FIX: Effect of Query Centering on NDCG@10\")\n",
    "print(\"=\" * 110)\n",
    "print(\"  Current: (q-m)^T sign(d-m)   →   Proposed: q^T [a · sign(d-m)]  (no centering)\\n\")\n",
    "\n",
    "results_fix = {}\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        d = data[(model_key, dataset)]\n",
    "        full_dim = d[\"corpus_emb\"].shape[1]\n",
    "        model_short = model_key.split(\"-\")[0]\n",
    "        dims = [full_dim, 256, 128, 64]\n",
    "        \n",
    "        print(f\"{'─'*60}\")\n",
    "        print(f\"  {model_short} / {dataset}\")\n",
    "        print(f\"{'─'*60}\")\n",
    "        print(f\"  {'Method':<28} \" + \" \".join(f\"{'d='+str(dm):>9}\" for dm in dims))\n",
    "        print(f\"  {'─'*28} \" + \"─\" * (10 * len(dims)))\n",
    "        \n",
    "        for name, fn in methods_fix:\n",
    "            print(f\"  {name:<28} \", end=\"\")\n",
    "            for dm in dims:\n",
    "                if dm > full_dim:\n",
    "                    print(f\"{'—':>9}\", end=\" \")\n",
    "                    continue\n",
    "                c = truncate_and_normalize(d[\"corpus_emb\"], dm)\n",
    "                q = truncate_and_normalize(d[\"query_emb\"], dm)\n",
    "                idx = fn(q, c)\n",
    "                m = evaluate_indices(idx, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "                ndcg = m.get('ndcg@10', 0)\n",
    "                results_fix[(model_key, dataset, name, dm)] = ndcg\n",
    "                print(f\"{ndcg:>9.4f}\", end=\" \")\n",
    "            print()\n",
    "        \n",
    "        # Float32 baseline\n",
    "        print(f\"  {'--- float32 ---':<28} \", end=\"\")\n",
    "        for dm in dims:\n",
    "            if dm > full_dim:\n",
    "                print(f\"{'—':>9}\", end=\" \")\n",
    "                continue\n",
    "            c = truncate_and_normalize(d[\"corpus_emb\"], dm)\n",
    "            q = truncate_and_normalize(d[\"query_emb\"], dm)\n",
    "            idx = search_topk(q @ c.T, 100)\n",
    "            m = evaluate_indices(idx, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "            print(f\"{m.get('ndcg@10',0):>9.4f}\", end=\" \")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# ─── Compute deltas ───\n",
    "print(\"=\" * 110)\n",
    "print(\"Δ NDCG@10 from removing query centering  (positive = no-center is better)\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        d_obj = data[(model_key, dataset)]\n",
    "        full_dim = d_obj[\"corpus_emb\"].shape[1]\n",
    "        model_short = model_key.split(\"-\")[0]\n",
    "        dims = [full_dim, 256, 128, 64]\n",
    "        \n",
    "        print(f\"\\n  {model_short} / {dataset}:\")\n",
    "        print(f\"  {'Comparison':<28} \" + \" \".join(f\"{'d='+str(dm):>9}\" for dm in dims))\n",
    "        for center_name, nocenter_name, label in [\n",
    "            (\"med_asym (center q)\", \"med_asym (NO center)\", \"unweighted median\"),\n",
    "            (\"w_med_asym (center q)\", \"w_med_asym (NO center)\", \"weighted median\"),\n",
    "        ]:\n",
    "            print(f\"    Δ {label:<24} \", end=\"\")\n",
    "            for dm in dims:\n",
    "                if dm > full_dim:\n",
    "                    print(f\"{'—':>9}\", end=\" \")\n",
    "                    continue\n",
    "                v_c = results_fix.get((model_key, dataset, center_name, dm), 0)\n",
    "                v_nc = results_fix.get((model_key, dataset, nocenter_name, dm), 0)\n",
    "                delta = v_nc - v_c\n",
    "                print(f\"{delta:>+9.4f}\", end=\" \")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── GPT 5.2 Test B+C: 2-Bit Sign-Magnitude & Lloyd-Max ───\n",
    "\n",
    "def run_sign_magnitude(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"2-bit sign-magnitude: 1 sign bit + 1 magnitude bit with conditional mean reconstruction.\"\"\"\n",
    "    medians = np.median(corpus_emb, axis=0)\n",
    "    centered = corpus_emb - medians\n",
    "    abs_c = np.abs(centered)\n",
    "    \n",
    "    signs = np.sign(centered)\n",
    "    signs[signs == 0] = 1  # handle exact zeros\n",
    "    \n",
    "    # Magnitude threshold: median of |d-m| per dim (balanced bits)\n",
    "    tau = np.median(abs_c, axis=0)\n",
    "    large_mask = abs_c > tau\n",
    "    small_mask = ~large_mask\n",
    "    \n",
    "    # Conditional reconstruction means (vectorized)\n",
    "    alpha_0 = np.where(small_mask.sum(0) > 0,\n",
    "                       (abs_c * small_mask).sum(0) / small_mask.sum(0).clip(1), 0).astype(np.float32)\n",
    "    alpha_1 = np.where(large_mask.sum(0) > 0,\n",
    "                       (abs_c * large_mask).sum(0) / large_mask.sum(0).clip(1), 0).astype(np.float32)\n",
    "    \n",
    "    # Reconstruct: sign * (alpha_0 when small, alpha_1 when large)\n",
    "    mag = large_mask.astype(np.float32)\n",
    "    recon = signs * (alpha_0 * (1 - mag) + alpha_1 * mag)\n",
    "    \n",
    "    # Score using uncentered query (q^T m is constant for ranking)\n",
    "    scores = query_emb @ recon.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "\n",
    "def run_lloyd_max(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"Gaussian-optimal 4-level Lloyd-Max quantization.\n",
    "    Thresholds: ±0.9816σ, 0   Levels: ±0.4528σ, ±1.5104σ\"\"\"\n",
    "    medians = np.median(corpus_emb, axis=0)\n",
    "    stds = np.std(corpus_emb, axis=0).clip(1e-10)\n",
    "    z = (corpus_emb - medians) / stds\n",
    "    \n",
    "    # Gaussian-optimal reconstruction levels\n",
    "    levels = np.array([-1.5104, -0.4528, 0.4528, 1.5104], dtype=np.float32)\n",
    "    codes = np.digitize(z, [-0.9816, 0, 0.9816])  # → {0, 1, 2, 3}\n",
    "    recon_z = levels[codes]\n",
    "    \n",
    "    # GPT 5.2 trick: score = (q·σ)^T recon_z  (q^T m is constant)\n",
    "    scores = (query_emb * stds) @ recon_z.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "\n",
    "def run_lloyd_max_empirical(query_emb, corpus_emb, k=100, n_iter=10):\n",
    "    \"\"\"Data-driven Lloyd-Max: run 1D k-means with 4 centroids per dimension.\"\"\"\n",
    "    dim = corpus_emb.shape[1]\n",
    "    medians = np.median(corpus_emb, axis=0)\n",
    "    stds = np.std(corpus_emb, axis=0).clip(1e-10)\n",
    "    z = (corpus_emb - medians) / stds\n",
    "    \n",
    "    # Initialize with Gaussian-optimal\n",
    "    all_levels = np.tile([-1.5104, -0.4528, 0.4528, 1.5104], (dim, 1)).astype(np.float32)  # (d, 4)\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        # Boundaries = midpoints between adjacent levels\n",
    "        bounds = (all_levels[:, :-1] + all_levels[:, 1:]) / 2  # (d, 3)\n",
    "        \n",
    "        # Assign codes: vectorized\n",
    "        codes = np.zeros_like(z, dtype=np.int64)\n",
    "        for b in range(3):\n",
    "            codes += (z > bounds[:, b]).astype(np.int64)\n",
    "        \n",
    "        # Update centroids\n",
    "        for c_val in range(4):\n",
    "            mask = (codes == c_val)\n",
    "            counts = mask.sum(axis=0).clip(1)\n",
    "            sums = (z * mask).sum(axis=0)\n",
    "            new_levels = sums / counts\n",
    "            has_data = mask.sum(axis=0) > 0\n",
    "            all_levels[has_data, c_val] = new_levels[has_data]\n",
    "    \n",
    "    # Final assignment\n",
    "    bounds = (all_levels[:, :-1] + all_levels[:, 1:]) / 2\n",
    "    codes = np.zeros_like(z, dtype=np.int64)\n",
    "    for b in range(3):\n",
    "        codes += (z > bounds[:, b]).astype(np.int64)\n",
    "    \n",
    "    # Reconstruct: recon_z[i, j] = all_levels[j, codes[i, j]]\n",
    "    dim_idx = np.arange(dim)[np.newaxis, :]  # (1, d) broadcasts with (n, d)\n",
    "    recon_z = all_levels[dim_idx, codes]  # (n, d)\n",
    "    \n",
    "    scores = (query_emb * stds) @ recon_z.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "\n",
    "# ─── Compare all 2-bit methods ───\n",
    "print(\"=\" * 115)\n",
    "print(\"2-BIT METHOD COMPARISON: All approaches at 2 bits/dim — NDCG@10\")\n",
    "print(\"=\" * 115)\n",
    "\n",
    "methods_2bit_all = [\n",
    "    (\"binary_med_asym (1b)\",           1, lambda q,c,k=100: run_binary_median_asym(q,c,k)),\n",
    "    (\"ternary_asym (2b, dead-zone)\",   2, lambda q,c,k=100: run_ternary_asym(q,c,k, use_median=True)),\n",
    "    (\"quaternary_asym (2b, quartile)\", 2, lambda q,c,k=100: run_quaternary_asym(q,c,k)),\n",
    "    (\"sign_magnitude (2b, GPT5.2)\",    2, run_sign_magnitude),\n",
    "    (\"lloyd_max_gauss (2b, GPT5.2)\",   2, run_lloyd_max),\n",
    "    (\"lloyd_max_empir (2b, GPT5.2)\",   2, run_lloyd_max_empirical),\n",
    "]\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        d = data[(model_key, dataset)]\n",
    "        full_dim = d[\"corpus_emb\"].shape[1]\n",
    "        model_short = model_key.split(\"-\")[0]\n",
    "        dims = [full_dim, 256, 128, 64]\n",
    "        \n",
    "        print(f\"\\n{'─'*65}\")\n",
    "        print(f\"  {model_short} / {dataset}\")\n",
    "        print(f\"{'─'*65}\")\n",
    "        print(f\"  {'Method':<35} \" + \" \".join(f\"{'d='+str(dm):>9}\" for dm in dims) + f\" {'bits':>5}\")\n",
    "        print(f\"  {'─'*35} \" + \"─\" * (10 * len(dims) + 6))\n",
    "        \n",
    "        for name, bits, fn in methods_2bit_all:\n",
    "            print(f\"  {name:<35} \", end=\"\")\n",
    "            for dm in dims:\n",
    "                if dm > full_dim:\n",
    "                    print(f\"{'—':>9}\", end=\" \")\n",
    "                    continue\n",
    "                c = truncate_and_normalize(d[\"corpus_emb\"], dm)\n",
    "                q = truncate_and_normalize(d[\"query_emb\"], dm)\n",
    "                idx = fn(q, c)\n",
    "                m = evaluate_indices(idx, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "                print(f\"{m.get('ndcg@10',0):>9.4f}\", end=\" \")\n",
    "            print(f\"{bits:>5}\")\n",
    "        \n",
    "        # Float32 baseline\n",
    "        print(f\"  {'--- float32 (32b) ---':<35} \", end=\"\")\n",
    "        for dm in dims:\n",
    "            if dm > full_dim:\n",
    "                print(f\"{'—':>9}\", end=\" \")\n",
    "                continue\n",
    "            c = truncate_and_normalize(d[\"corpus_emb\"], dm)\n",
    "            q = truncate_and_normalize(d[\"query_emb\"], dm)\n",
    "            idx = search_topk(q @ c.T, 100)\n",
    "            m = evaluate_indices(idx, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "            print(f\"{m.get('ndcg@10',0):>9.4f}\", end=\" \")\n",
    "        print(f\"{'32':>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859364a",
   "metadata": {},
   "source": [
    "## 10. The Lloyd-Max Anomaly — Why Does 2-Bit Beat Float32?\n",
    "\n",
    "Lloyd-Max at 2 bits gives NDCG@10 = 0.7465 on mxbai/scifact, **exceeding** float32's 0.7389. Three hypotheses:\n",
    "\n",
    "**A. σ-weighting:** The scoring trick `(q·σ)^T recon_z` implicitly re-weights dimensions by variance, which could be a *better* similarity measure than raw inner product.\n",
    "\n",
    "**B. Denoising:** Quantization collapses nearby values to cluster centers, acting like a regularizer that removes per-dimension noise.\n",
    "\n",
    "**C. Score-gap amplification:** Quantization may increase the gap between relevant and irrelevant document scores, improving ranking.\n",
    "\n",
    "### Experiment: 5 conditions to disentangle the factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Q9: DISENTANGLING THE LLOYD-MAX ANOMALY\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# 5 conditions: \n",
    "#   C1: float32 standard inner product (baseline)\n",
    "#   C2: float32 with σ-weighted inner product (test hypothesis A)\n",
    "#   C3: Lloyd-Max with σ-folding (current best — A+B together)\n",
    "#   C4: Lloyd-Max WITHOUT σ-folding (test hypothesis B alone)\n",
    "#   C5: float32 + Gaussian noise (test noise sensitivity)\n",
    "\n",
    "from scipy.stats import kurtosis as scipy_kurtosis\n",
    "\n",
    "def run_float32_sigma_weighted(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"C2: Float32 with σ-weighted inner product (no quantization).\"\"\"\n",
    "    stds = np.std(corpus_emb, axis=0).clip(1e-10)\n",
    "    # Weight both sides by σ: score = (q·σ)^T (d·σ) \n",
    "    # But that changes the metric. Instead, match Lloyd-Max's scoring:\n",
    "    # Lloyd-Max scores as (q·σ)^T recon_z, which scales query by σ\n",
    "    # Equivalent: weight each dim by σ²\n",
    "    weighted_corpus = corpus_emb * stds  # d·σ\n",
    "    weighted_query = query_emb * stds    # q·σ\n",
    "    scores = weighted_query @ weighted_corpus.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "def run_lloyd_max_no_sigma_fold(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"C4: Lloyd-Max reconstruction but WITHOUT σ-folding in scoring.\n",
    "    Reconstructs d_hat = median + σ * L[code], then scores as q^T d_hat.\"\"\"\n",
    "    medians = np.median(corpus_emb, axis=0)\n",
    "    stds = np.std(corpus_emb, axis=0).clip(1e-10)\n",
    "    z = (corpus_emb - medians) / stds\n",
    "    \n",
    "    levels = np.array([-1.5104, -0.4528, 0.4528, 1.5104], dtype=np.float32)\n",
    "    codes = np.digitize(z, [-0.9816, 0, 0.9816])\n",
    "    recon_z = levels[codes]\n",
    "    \n",
    "    # Reconstruct in original space: d_hat = median + σ * recon_z\n",
    "    recon = medians + stds * recon_z\n",
    "    scores = query_emb @ recon.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "def run_float32_noisy(query_emb, corpus_emb, k=100, noise_level=0.01):\n",
    "    \"\"\"C5: Float32 with added Gaussian noise to test noise sensitivity.\"\"\"\n",
    "    rng = np.random.default_rng(42)\n",
    "    noisy_corpus = corpus_emb + rng.normal(0, noise_level, corpus_emb.shape).astype(np.float32)\n",
    "    # Re-normalize (since embeddings are L2-normalized)\n",
    "    noisy_corpus = noisy_corpus / np.linalg.norm(noisy_corpus, axis=1, keepdims=True)\n",
    "    scores = query_emb @ noisy_corpus.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"Q9: DISENTANGLING THE LLOYD-MAX ANOMALY\")\n",
    "print(\"Why does 2-bit Lloyd-Max beat float32?\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "conditions = [\n",
    "    (\"C1: float32 (baseline)\",      lambda q,c: search_topk(q @ c.T, 100)),\n",
    "    (\"C2: float32 σ²-weighted\",     run_float32_sigma_weighted),\n",
    "    (\"C3: lloyd_max + σ-fold\",      run_lloyd_max),         # current best\n",
    "    (\"C4: lloyd_max NO σ-fold\",     run_lloyd_max_no_sigma_fold),\n",
    "    (\"C5: float32 + noise(0.01)\",   lambda q,c: run_float32_noisy(q,c, noise_level=0.01)),\n",
    "    (\"C5b: float32 + noise(0.05)\",  lambda q,c: run_float32_noisy(q,c, noise_level=0.05)),\n",
    "]\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        d = data[(model_key, dataset)]\n",
    "        full_dim = d[\"corpus_emb\"].shape[1]\n",
    "        model_short = model_key.split(\"-\")[0]\n",
    "        dims = [full_dim, 256, 128]\n",
    "        \n",
    "        print(f\"\\n{'─'*70}\")\n",
    "        print(f\"  {model_short} / {dataset}\")\n",
    "        print(f\"{'─'*70}\")\n",
    "        print(f\"  {'Condition':<32} \" + \" \".join(f\"{'d='+str(dm):>9}\" for dm in dims))\n",
    "        print(f\"  {'─'*32} \" + \"─\" * (10 * len(dims)))\n",
    "        \n",
    "        for name, fn in conditions:\n",
    "            print(f\"  {name:<32} \", end=\"\")\n",
    "            for dm in dims:\n",
    "                if dm > full_dim:\n",
    "                    print(f\"{'—':>9}\", end=\" \")\n",
    "                    continue\n",
    "                c = truncate_and_normalize(d[\"corpus_emb\"], dm)\n",
    "                q = truncate_and_normalize(d[\"query_emb\"], dm)\n",
    "                idx = fn(q, c)\n",
    "                m = evaluate_indices(idx, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "                print(f\"{m.get('ndcg@10',0):>9.4f}\", end=\" \")\n",
    "            print()\n",
    "        \n",
    "        # Also compute per-dim kurtosis as a feature\n",
    "        c_full = truncate_and_normalize(d[\"corpus_emb\"], full_dim)\n",
    "        k_vals = scipy_kurtosis(c_full, axis=0, fisher=True)  # excess kurtosis\n",
    "        print(f\"\\n  Per-dim statistics:\")\n",
    "        print(f\"    Mean excess kurtosis:   {np.mean(k_vals):>8.4f}  (Gaussian = 0)\")\n",
    "        print(f\"    Std of kurtosis:        {np.std(k_vals):>8.4f}\")\n",
    "        print(f\"    Mean |skewness|:        {np.mean(np.abs(np.mean((c_full - c_full.mean(0))**3, axis=0) / np.std(c_full, axis=0)**3)):>8.4f}\")\n",
    "        print(f\"    Effective dimensionality: {(np.sum(np.var(c_full, axis=0))**2 / np.sum(np.var(c_full, axis=0)**2)):>8.1f} / {full_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5dfb50",
   "metadata": {},
   "source": [
    "### Q9 Findings\n",
    "\n",
    "**Key result: C3 = C4 exactly.** The σ-folding scoring trick doesn't change rankings — `(q·σ)^T L[codes]` differs from `q^T (m + σ·L[codes])` only by the constant `q^T m`, which doesn't affect ranking.\n",
    "\n",
    "**The anomaly is dataset-specific.** Lloyd-Max beats float32 only on mxbai/scifact (+0.0076 NDCG). On all other configs it *hurts*. This rules out a universal mechanism.\n",
    "\n",
    "**σ-weighting (C2) barely helps.** Effective dimensionality is 992/1024 (mxbai) and 739/768 (nomic), confirming near-uniform per-dim variance. There's almost nothing to re-weight.\n",
    "\n",
    "**The gain on mxbai/scifact is from denoising (Hypothesis B).** The quantization itself (collapsing to 4 discrete levels) acts as a regularizer. SciFact is a small dataset (5K docs, 300 queries) where quantization may denoise overfitted embedding dimensions.\n",
    "\n",
    "**Noise sensitivity confirms embeddings are somewhat noisy:** 1% noise barely hurts, but 5% noise is catastrophic.\n",
    "\n",
    "## 11. New Methods — Residual Quantization, Rotation + Lloyd-Max, 3-Bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c02b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Q11: RESIDUAL 1+1 BIT QUANTIZATION\n",
    "# Q12: ROTATION + LLOYD-MAX  \n",
    "# Q13: 3-BIT LLOYD-MAX (8 levels)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# --- Residual quantization: 2 binary stages ---\n",
    "def run_residual_1plus1(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"Two-stage binary residual quantization (2 bits/dim total).\n",
    "    \n",
    "    Stage 1: binary_med_asym → captures sign relative to median\n",
    "    Stage 2: quantize the residual (d - reconstruct(stage1)) at 1 bit\n",
    "    \n",
    "    Total: 2 bits/dim, same storage as Lloyd-Max.\n",
    "    \"\"\"\n",
    "    n, dim = corpus_emb.shape\n",
    "    \n",
    "    # Stage 1: median-centered binary\n",
    "    med = np.median(corpus_emb, axis=0)\n",
    "    centered = corpus_emb - med\n",
    "    signs = (centered > 0).astype(np.float32) * 2 - 1  # {-1, +1}\n",
    "    \n",
    "    # Conditional mean reconstruction for stage 1\n",
    "    pos_mask = centered > 0\n",
    "    neg_mask = ~pos_mask\n",
    "    alpha_pos = np.where(pos_mask.sum(0) > 0,\n",
    "                         (centered * pos_mask).sum(0) / pos_mask.sum(0).clip(1), 0).astype(np.float32)\n",
    "    alpha_neg = np.where(neg_mask.sum(0) > 0,\n",
    "                         (centered * neg_mask).sum(0) / neg_mask.sum(0).clip(1), 0).astype(np.float32)\n",
    "    \n",
    "    recon_stage1 = np.where(pos_mask, alpha_pos, alpha_neg)\n",
    "    \n",
    "    # Residual after stage 1\n",
    "    residual = centered - recon_stage1\n",
    "    \n",
    "    # Stage 2: binary quantize the residual\n",
    "    med_r = np.median(residual, axis=0)\n",
    "    resid_centered = residual - med_r\n",
    "    signs2 = (resid_centered > 0).astype(np.float32) * 2 - 1\n",
    "    \n",
    "    pos2 = resid_centered > 0\n",
    "    neg2 = ~pos2\n",
    "    beta_pos = np.where(pos2.sum(0) > 0,\n",
    "                        (resid_centered * pos2).sum(0) / pos2.sum(0).clip(1), 0).astype(np.float32)\n",
    "    beta_neg = np.where(neg2.sum(0) > 0,\n",
    "                        (resid_centered * neg2).sum(0) / neg2.sum(0).clip(1), 0).astype(np.float32)\n",
    "    \n",
    "    recon_stage2 = np.where(pos2, beta_pos, beta_neg)\n",
    "    \n",
    "    # Full reconstruction: med + recon_stage1 + med_r + recon_stage2\n",
    "    full_recon = med + recon_stage1 + med_r + recon_stage2\n",
    "    \n",
    "    scores = query_emb @ full_recon.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "\n",
    "# --- Rotation + Lloyd-Max ---\n",
    "def random_orthogonal_matrix(d, seed=42):\n",
    "    \"\"\"Generate a random orthogonal matrix via QR decomposition.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    G = rng.standard_normal((d, d)).astype(np.float32)\n",
    "    Q, R = np.linalg.qr(G)\n",
    "    # Fix sign ambiguity\n",
    "    Q *= np.sign(np.diag(R))\n",
    "    return Q\n",
    "\n",
    "def run_rotation_lloyd_max(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"Apply random rotation before Lloyd-Max to make dims more Gaussian.\"\"\"\n",
    "    dim = corpus_emb.shape[1]\n",
    "    R = random_orthogonal_matrix(dim)\n",
    "    \n",
    "    # Rotate both corpus and query\n",
    "    corpus_rot = corpus_emb @ R\n",
    "    query_rot = query_emb @ R\n",
    "    \n",
    "    # Then standard Lloyd-Max\n",
    "    medians = np.median(corpus_rot, axis=0)\n",
    "    stds = np.std(corpus_rot, axis=0).clip(1e-10)\n",
    "    z = (corpus_rot - medians) / stds\n",
    "    \n",
    "    levels = np.array([-1.5104, -0.4528, 0.4528, 1.5104], dtype=np.float32)\n",
    "    codes = np.digitize(z, [-0.9816, 0, 0.9816])\n",
    "    recon_z = levels[codes]\n",
    "    \n",
    "    scores = (query_rot * stds) @ recon_z.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "\n",
    "# --- 3-bit Lloyd-Max (8 levels) ---\n",
    "def run_lloyd_max_3bit(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"3-bit Gaussian-optimal Lloyd-Max quantizer (8 levels).\n",
    "    \n",
    "    Optimal boundaries and reconstruction levels for N(0,1):\n",
    "    Boundaries: [-1.7480, -1.0500, -0.5006, 0, 0.5006, 1.0500, 1.7480]\n",
    "    Levels:     [-2.1520, -1.3440, -0.7560, -0.2451, 0.2451, 0.7560, 1.3440, 2.1520]\n",
    "    \"\"\"\n",
    "    medians = np.median(corpus_emb, axis=0)\n",
    "    stds = np.std(corpus_emb, axis=0).clip(1e-10)\n",
    "    z = (corpus_emb - medians) / stds\n",
    "    \n",
    "    boundaries = np.array([-1.7480, -1.0500, -0.5006, 0, 0.5006, 1.0500, 1.7480])\n",
    "    levels = np.array([-2.1520, -1.3440, -0.7560, -0.2451, 0.2451, 0.7560, 1.3440, 2.1520], dtype=np.float32)\n",
    "    \n",
    "    codes = np.digitize(z, boundaries)  # → {0, 1, ..., 7}\n",
    "    recon_z = levels[codes]\n",
    "    \n",
    "    scores = (query_emb * stds) @ recon_z.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "\n",
    "# --- ITQ: Iterative Quantization for binary ---\n",
    "def learn_itq_rotation(corpus_emb, n_iter=50, seed=42):\n",
    "    \"\"\"Learn a rotation matrix R that minimizes binary quantization error.\n",
    "    min_R ||sign(X@R) - X@R||_F^2  s.t. R^T R = I\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    dim = corpus_emb.shape[1]\n",
    "    \n",
    "    # Initialize with random rotation\n",
    "    R = random_orthogonal_matrix(dim, seed)\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        # Rotate\n",
    "        V = corpus_emb @ R\n",
    "        # Binarize\n",
    "        B = np.sign(V)\n",
    "        B[B == 0] = 1\n",
    "        # Procrustes: find R that best aligns B with X\n",
    "        # B^T @ X = U S V^T → R = V U^T\n",
    "        S = B.T @ corpus_emb  # (dim, dim)\n",
    "        U, _, Vt = np.linalg.svd(S, full_matrices=True)\n",
    "        R = (Vt.T @ U.T).astype(np.float32)\n",
    "    \n",
    "    return R\n",
    "\n",
    "def run_itq_binary_asym(query_emb, corpus_emb, k=100, R=None):\n",
    "    \"\"\"Binary asymmetric search with ITQ-learned rotation.\"\"\"\n",
    "    if R is None:\n",
    "        R = learn_itq_rotation(corpus_emb)\n",
    "    \n",
    "    corpus_rot = corpus_emb @ R\n",
    "    query_rot = query_emb @ R\n",
    "    \n",
    "    # Median-centered binary asymmetric\n",
    "    med = np.median(corpus_rot, axis=0)\n",
    "    binary = np.packbits(((corpus_rot - med) > 0).astype(np.uint8), axis=1)\n",
    "    dim = query_emb.shape[1]\n",
    "    unpacked = np.unpackbits(binary, axis=1)[:, :dim].astype(np.float32)\n",
    "    unpacked = 2.0 * unpacked - 1.0\n",
    "    \n",
    "    scores = (query_rot - med) @ unpacked.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Run comparison of ALL methods (existing + new)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"COMPREHENSIVE METHOD COMPARISON — Existing + New Methods\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "all_methods = [\n",
    "    (\"binary_med_asym (1b)\",           1,  lambda q,c: run_binary_median_asym(q,c)),\n",
    "    (\"quaternary_asym (2b)\",           2,  lambda q,c: run_quaternary_asym(q,c)),\n",
    "    (\"lloyd_max_gauss (2b)\",           2,  run_lloyd_max),\n",
    "    (\"residual_1+1 (2b) [NEW]\",       2,  run_residual_1plus1),\n",
    "    (\"rotation+lloyd_max (2b) [NEW]\",  2,  run_rotation_lloyd_max),\n",
    "    (\"lloyd_max_3bit (3b) [NEW]\",      3,  run_lloyd_max_3bit),\n",
    "    (\"itq+binary_med (1b) [NEW]\",      1,  run_itq_binary_asym),\n",
    "    (\"int8_asym (8b)\",                 8,  lambda q,c: search_topk((q @ (((c - c.min(0)) / (c.max(0) - c.min(0)).clip(1e-10) * 255).round() / 255 * (c.max(0) - c.min(0)) + c.min(0)).T), 100)),\n",
    "    (\"float32 (32b)\",                 32,  lambda q,c: search_topk(q @ c.T, 100)),\n",
    "]\n",
    "\n",
    "# Simpler int8 implementation\n",
    "def run_int8_asym(query_emb, corpus_emb, k=100):\n",
    "    \"\"\"Standard int8 asymmetric: quantize corpus to 256 levels per dim.\"\"\"\n",
    "    mins = corpus_emb.min(axis=0)\n",
    "    maxs = corpus_emb.max(axis=0)\n",
    "    ranges = (maxs - mins).clip(1e-10)\n",
    "    # Quantize to [0, 255] and reconstruct\n",
    "    codes = np.round((corpus_emb - mins) / ranges * 255).clip(0, 255)\n",
    "    recon = codes / 255 * ranges + mins\n",
    "    scores = query_emb @ recon.T\n",
    "    return search_topk(scores, k)\n",
    "\n",
    "# Fix the list with proper int8\n",
    "all_methods = [\n",
    "    (\"binary_med_asym (1b)\",           1,  lambda q,c: run_binary_median_asym(q,c)),\n",
    "    (\"itq+binary_med (1b) [NEW]\",      1,  run_itq_binary_asym),\n",
    "    (\"quaternary_asym (2b)\",           2,  lambda q,c: run_quaternary_asym(q,c)),\n",
    "    (\"lloyd_max_gauss (2b)\",           2,  run_lloyd_max),\n",
    "    (\"residual_1+1 (2b) [NEW]\",       2,  run_residual_1plus1),\n",
    "    (\"rotation+lloyd_max (2b) [NEW]\",  2,  run_rotation_lloyd_max),\n",
    "    (\"lloyd_max_3bit (3b) [NEW]\",      3,  run_lloyd_max_3bit),\n",
    "    (\"int8_asym (8b)\",                 8,  run_int8_asym),\n",
    "    (\"float32 (32b)\",                 32,  lambda q,c: search_topk(q @ c.T, 100)),\n",
    "]\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        d = data[(model_key, dataset)]\n",
    "        full_dim = d[\"corpus_emb\"].shape[1]\n",
    "        model_short = model_key.split(\"-\")[0]\n",
    "        dims = [full_dim, 256, 128]\n",
    "        \n",
    "        print(f\"\\n{'═'*80}\")\n",
    "        print(f\"  {model_short} / {dataset} (full_dim={full_dim})\")\n",
    "        print(f\"{'═'*80}\")\n",
    "        print(f\"  {'Method':<36} \" + \" \".join(f\"{'d='+str(dm):>9}\" for dm in dims) + f\" {'bits':>5}\")\n",
    "        print(f\"  {'─'*36} \" + \"─\" * (10 * len(dims) + 6))\n",
    "        \n",
    "        for name, bits, fn in all_methods:\n",
    "            print(f\"  {name:<36} \", end=\"\", flush=True)\n",
    "            for dm in dims:\n",
    "                if dm > full_dim:\n",
    "                    print(f\"{'—':>9}\", end=\" \")\n",
    "                    continue\n",
    "                c = truncate_and_normalize(d[\"corpus_emb\"], dm)\n",
    "                q = truncate_and_normalize(d[\"query_emb\"], dm)\n",
    "                try:\n",
    "                    idx = fn(q, c)\n",
    "                    m = evaluate_indices(idx, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "                    print(f\"{m.get('ndcg@10',0):>9.4f}\", end=\" \")\n",
    "                except Exception as e:\n",
    "                    print(f\"{'ERR':>9}\", end=\" \")\n",
    "            print(f\"{bits:>5}\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 120)\n",
    "print(\"PARETO ANALYSIS: Storage (bytes/vec) vs NDCG@10\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Compute storage and build Pareto analysis\n",
    "for model_key in MODELS_TO_TEST[:1]:  # Just mxbai for conciseness\n",
    "    for dataset in DATASETS_TO_TEST[:1]:  # Just scifact\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        d = data[(model_key, dataset)]\n",
    "        full_dim = d[\"corpus_emb\"].shape[1]\n",
    "        \n",
    "        pareto_points = []\n",
    "        storage_methods = [\n",
    "            (\"binary_med_asym\",     1, lambda q,c: run_binary_median_asym(q,c)),\n",
    "            (\"itq+binary_med\",      1, run_itq_binary_asym),\n",
    "            (\"quaternary_asym\",     2, lambda q,c: run_quaternary_asym(q,c)),\n",
    "            (\"lloyd_max\",           2, run_lloyd_max),\n",
    "            (\"residual_1+1\",        2, run_residual_1plus1),\n",
    "            (\"rot+lloyd_max\",       2, run_rotation_lloyd_max),\n",
    "            (\"lloyd_max_3bit\",      3, run_lloyd_max_3bit),\n",
    "            (\"int8_asym\",           8, run_int8_asym),\n",
    "            (\"float32\",            32, lambda q,c: search_topk(q @ c.T, 100)),\n",
    "        ]\n",
    "        \n",
    "        dims_list = [64, 128, 256, 512, full_dim]\n",
    "        \n",
    "        for name, bits, fn in storage_methods:\n",
    "            for dm in dims_list:\n",
    "                if dm > full_dim:\n",
    "                    continue\n",
    "                c = truncate_and_normalize(d[\"corpus_emb\"], dm)\n",
    "                q = truncate_and_normalize(d[\"query_emb\"], dm)\n",
    "                bytes_per_vec = dm * bits / 8\n",
    "                try:\n",
    "                    idx = fn(q, c)\n",
    "                    m = evaluate_indices(idx, d[\"qrels\"], d[\"doc_id_to_idx\"], d[\"query_ids\"])\n",
    "                    ndcg = m.get(\"ndcg@10\", 0)\n",
    "                    pareto_points.append((bytes_per_vec, ndcg, f\"{name} d={dm}\"))\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Sort by storage, extract Pareto front\n",
    "        pareto_points.sort(key=lambda x: (x[0], -x[1]))\n",
    "        pareto_front = []\n",
    "        max_ndcg = 0\n",
    "        for bpv, ndcg, label in pareto_points:\n",
    "            if ndcg > max_ndcg:\n",
    "                pareto_front.append((bpv, ndcg, label))\n",
    "                max_ndcg = ndcg\n",
    "        \n",
    "        print(f\"\\n{'─'*90}\")\n",
    "        print(f\"  Pareto-optimal methods (mxbai / scifact)\")\n",
    "        print(f\"{'─'*90}\")\n",
    "        print(f\"  {'Method':<40} {'Bytes/vec':>10} {'NDCG@10':>9} {'% float32':>10}\")\n",
    "        print(f\"  {'─'*40} {'─'*10} {'─'*9} {'─'*10}\")\n",
    "        \n",
    "        f32_ndcg = [n for b,n,l in pareto_points if 'float32' in l and 'd='+str(full_dim) in l][0]\n",
    "        for bpv, ndcg, label in pareto_front:\n",
    "            pct = ndcg / f32_ndcg * 100\n",
    "            marker = \" ★\" if \"[NEW]\" in label or any(x in label for x in [\"residual\", \"rot+\", \"3bit\", \"itq\"]) else \"\"\n",
    "            print(f\"  {label:<40} {bpv:>10.0f} {ndcg:>9.4f} {pct:>9.1f}%{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638aa956",
   "metadata": {},
   "source": [
    "### Results Summary — New Methods\n",
    "\n",
    "**Residual 1+1 bit is the new champion on mxbai/scifact** (0.7486 > 0.7465 Lloyd-Max > 0.7389 float32). At the same 2 bits/dim, it captures both the sign and magnitude refinement through successive approximation.\n",
    "\n",
    "**Lloyd-Max 3-bit is surprisingly strong.** At d=128, it reaches 0.6552 (97.6% of float32) at only 48 bytes — compared to int8_asym's 0.6727 at 128 bytes. It enters the Pareto front at multiple points.\n",
    "\n",
    "**ITQ rotation hurts at full dim** but marginally helps nomic at lower dims. The learned rotation doesn't beat simple median centering.\n",
    "\n",
    "**Rotation + Lloyd-Max** helps on nfcorpus (multi-label) and at low dims for nomic (0.5709 vs 0.5353 at d=128), but hurts on mxbai/scifact at full dim.\n",
    "\n",
    "### Updated Pareto front (mxbai/scifact, new entries marked ★):\n",
    "| Bytes | Method | NDCG@10 | % of f32 |\n",
    "|-------|--------|---------|----------|\n",
    "| 8 | binary_med_asym d=64 | 0.4536 | 61.4% |\n",
    "| 16 | binary_med_asym d=128 | 0.5688 | 77.0% |\n",
    "| 32 | binary_med_asym d=256 | 0.6534 | 88.4% |\n",
    "| 48 | ★ lloyd_max_3bit d=128 | 0.6552 | 88.7% |\n",
    "| 64 | binary_med_asym d=512 | 0.7031 | 95.2% |\n",
    "| 128 | binary_med_asym d=1024 | 0.7259 | 98.2% |\n",
    "| 192 | ★ lloyd_max_3bit d=512 | 0.7295 | 98.7% |\n",
    "| 256 | ★ **residual_1+1 d=1024** | **0.7486** | **101.3%** |\n",
    "\n",
    "## 12. Embedding Feature Analysis — Predicting When Methods Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Q14: EMBEDDING FEATURE ANALYSIS — What predicts quantization quality?\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "from scipy.stats import kurtosis as scipy_kurtosis, shapiro\n",
    "\n",
    "def compute_embedding_features(corpus_emb, label=\"\"):\n",
    "    \"\"\"Compute diagnostic features of an embedding distribution.\"\"\"\n",
    "    n, dim = corpus_emb.shape\n",
    "    \n",
    "    # 1. Kurtosis (excess, Gaussian = 0)\n",
    "    kurt = scipy_kurtosis(corpus_emb, axis=0, fisher=True)\n",
    "    \n",
    "    # 2. Sign imbalance\n",
    "    frac_pos = np.mean(corpus_emb > 0, axis=0)\n",
    "    sign_imbal = np.abs(frac_pos - 0.5)\n",
    "    \n",
    "    # 3. Effective dimensionality (participation ratio)\n",
    "    variances = np.var(corpus_emb, axis=0)\n",
    "    d_eff = np.sum(variances)**2 / np.sum(variances**2)\n",
    "    \n",
    "    # 4. Inter-dimension correlation (sample 200 pairs)\n",
    "    rng = np.random.default_rng(42)\n",
    "    n_pairs = min(200, dim * (dim - 1) // 2)\n",
    "    pairs = rng.choice(dim, size=(n_pairs, 2), replace=True)\n",
    "    corrs = [np.abs(np.corrcoef(corpus_emb[:, i], corpus_emb[:, j])[0, 1]) \n",
    "             for i, j in pairs if i != j]\n",
    "    mean_abs_corr = np.mean(corrs) if corrs else 0\n",
    "    \n",
    "    # 5. Gaussianity: Shapiro-Wilk on subsample of dims\n",
    "    subsample = corpus_emb[:min(500, n)]\n",
    "    n_gauss_dims = 0\n",
    "    for d_idx in range(min(20, dim)):\n",
    "        _, p = shapiro(subsample[:, d_idx])\n",
    "        if p > 0.05:\n",
    "            n_gauss_dims += 1\n",
    "    gauss_frac = n_gauss_dims / min(20, dim)\n",
    "    \n",
    "    # 6. Variance uniformity\n",
    "    var_cv = np.std(variances) / np.mean(variances)\n",
    "    \n",
    "    # 7. Median shift magnitude\n",
    "    medians = np.median(corpus_emb, axis=0)\n",
    "    stds = np.std(corpus_emb, axis=0)\n",
    "    med_shift = np.mean(np.abs(medians) / stds.clip(1e-10))\n",
    "    \n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"dim\": dim,\n",
    "        \"n\": n,\n",
    "        \"mean_kurtosis\": float(np.mean(kurt)),\n",
    "        \"std_kurtosis\": float(np.std(kurt)),\n",
    "        \"mean_sign_imbalance\": float(np.mean(sign_imbal)),\n",
    "        \"d_effective\": float(d_eff),\n",
    "        \"d_eff_ratio\": float(d_eff / dim),\n",
    "        \"mean_abs_corr\": float(mean_abs_corr),\n",
    "        \"gaussian_fraction\": float(gauss_frac),\n",
    "        \"variance_cv\": float(var_cv),\n",
    "        \"median_shift\": float(med_shift),\n",
    "    }\n",
    "\n",
    "# ─── Direct file-based loading (handles mixed-case filenames) ───\n",
    "def find_embedding_file(model_key, dataset, split=\"corpus\"):\n",
    "    \"\"\"Find embedding file trying different case variants.\"\"\"\n",
    "    for variant in [dataset, dataset.lower(), dataset.capitalize(), dataset.upper()]:\n",
    "        f = EMBEDDING_DIR / f\"{model_key}_{variant}_{split}.npy\"\n",
    "        if f.exists():\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "print(\"=\" * 110)\n",
    "print(\"Q14: EMBEDDING FEATURE ANALYSIS\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "all_model_keys = list(MODELS.keys())  # from config: mxbai, nomic, minilm\n",
    "datasets_to_analyze = [\"scifact\", \"nfcorpus\"]  # skip fiqa\n",
    "feature_table = []\n",
    "\n",
    "for model_key in all_model_keys:\n",
    "    for dataset in datasets_to_analyze:\n",
    "        corpus_file = find_embedding_file(model_key, dataset, \"corpus\")\n",
    "        if corpus_file is None:\n",
    "            print(f\"  ✗ No embeddings for {model_key}/{dataset}\")\n",
    "            continue\n",
    "        \n",
    "        emb = np.load(str(corpus_file)).astype(np.float32)\n",
    "        norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        emb = emb / norms.clip(1e-10)\n",
    "        \n",
    "        label = f\"{model_key.split('-')[0]}/{dataset}\"\n",
    "        features = compute_embedding_features(emb, label)\n",
    "        feature_table.append(features)\n",
    "\n",
    "print(f\"\\n  {'Config':<22} {'dim':>5} {'n':>7} {'κ̄':>7} {'imbal':>7} {'d_eff':>7} {'d_eff%':>7} {'|corr|':>7} {'gauss%':>7} {'var_cv':>7} {'med_sh':>7}\")\n",
    "print(f\"  {'─'*22} \" + \"─\" * 77)\n",
    "\n",
    "for f in feature_table:\n",
    "    print(f\"  {f['label']:<22} {f['dim']:>5} {f['n']:>7} {f['mean_kurtosis']:>7.3f} {f['mean_sign_imbalance']:>7.4f} \"\n",
    "          f\"{f['d_effective']:>7.0f} {f['d_eff_ratio']:>7.3f} {f['mean_abs_corr']:>7.4f} {f['gaussian_fraction']:>7.1%} \"\n",
    "          f\"{f['variance_cv']:>7.4f} {f['median_shift']:>7.4f}\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# FULL GRID: All models × datasets × key methods at full dim\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "print(\"\\n\\n\" + \"=\" * 110)\n",
    "print(\"FULL GRID: All models × All datasets × Key methods at full dim (skip fiqa)\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "key_methods = [\n",
    "    (\"binary_med_asym\", lambda q,c: run_binary_median_asym(q,c)),\n",
    "    (\"lloyd_max_gauss\", run_lloyd_max),\n",
    "    (\"residual_1+1\",    run_residual_1plus1),\n",
    "    (\"float32\",         lambda q,c: search_topk(q @ c.T, 100)),\n",
    "]\n",
    "\n",
    "print(f\"\\n  {'Config':<22} \" + \" \".join(f\"{n:>18}\" for n, _ in key_methods) + f\" {'LM-f32 delta':>14}\")\n",
    "print(f\"  {'─'*22} \" + \"─\" * (19 * len(key_methods) + 15))\n",
    "\n",
    "for model_key in all_model_keys:\n",
    "    for dataset in datasets_to_analyze:\n",
    "        corpus_file = find_embedding_file(model_key, dataset, \"corpus\")\n",
    "        query_file = find_embedding_file(model_key, dataset, \"queries\")\n",
    "        if corpus_file is None or query_file is None:\n",
    "            continue\n",
    "        \n",
    "        corpus_emb = np.load(str(corpus_file)).astype(np.float32)\n",
    "        query_emb = np.load(str(query_file)).astype(np.float32)\n",
    "        corpus_emb = corpus_emb / np.linalg.norm(corpus_emb, axis=1, keepdims=True).clip(1e-10)\n",
    "        query_emb = query_emb / np.linalg.norm(query_emb, axis=1, keepdims=True).clip(1e-10)\n",
    "        \n",
    "        # Load qrels\n",
    "        try:\n",
    "            corpus_data_local, queries_data_local, qrels_local = data_loader.load_dataset(dataset)\n",
    "            doc_ids_local = list(corpus_data_local.keys())\n",
    "            query_ids_local = list(queries_data_local.keys())\n",
    "            doc_id_to_idx_local = {did: i for i, did in enumerate(doc_ids_local)}\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {model_key.split('-')[0]}/{dataset}: qrels error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        label = f\"{model_key.split('-')[0]}/{dataset}\"\n",
    "        print(f\"  {label:<22} \", end=\"\", flush=True)\n",
    "        \n",
    "        results_row = {}\n",
    "        for method_name, fn in key_methods:\n",
    "            idx = fn(query_emb, corpus_emb)\n",
    "            m = evaluate_indices(idx, qrels_local, doc_id_to_idx_local, query_ids_local)\n",
    "            ndcg = m.get(\"ndcg@10\", 0)\n",
    "            results_row[method_name] = ndcg\n",
    "            print(f\"{ndcg:>18.4f}\", end=\" \")\n",
    "        \n",
    "        delta = results_row.get(\"lloyd_max_gauss\", 0) - results_row.get(\"float32\", 0)\n",
    "        print(f\"{delta:>+14.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf23ef",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Calibration Experiments (Q15)\n",
    "\n",
    "**Problem**: Vespa uses streaming search — documents are indexed one at a time and the corpus grows over time. Calibration statistics (medians, stds) computed at index creation may drift as new documents arrive.\n",
    "\n",
    "**Experiments**:\n",
    "1. **Cross-corpus calibration**: Calibrate on corpus A, evaluate on corpus B (same model). Tests if stats are model-intrinsic.\n",
    "2. **Small-sample calibration**: How many vectors are needed for stable calibration?\n",
    "3. **Synthetic drift**: Shift embeddings gradually and test with stale calibration.\n",
    "4. **Improved calibration-free**: Can we close the gap vs Lloyd-Max without corpus-specific stats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Q15a: Cross-Corpus Calibration — Are statistics model-intrinsic?\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "#\n",
    "# If per-dim medians/stds are mainly a property of the MODEL (not the corpus),\n",
    "# then we can calibrate once on any reference corpus and use it forever.\n",
    "# This eliminates the drift problem entirely.\n",
    "\n",
    "LLOYD_BOUNDS = np.array([-0.9816, 0.0, 0.9816])\n",
    "LLOYD_LEVELS = np.array([-1.5104, -0.4528, 0.4528, 1.5104], dtype=np.float32)\n",
    "\n",
    "def lloyd_max_search(queries, corpus, medians, stds):\n",
    "    \"\"\"Score queries against Lloyd-Max quantized corpus.\"\"\"\n",
    "    # Standardize and quantize corpus\n",
    "    z = (corpus - medians) / np.clip(stds, 1e-10, None)\n",
    "    codes = np.digitize(z, LLOYD_BOUNDS).astype(np.uint8)\n",
    "    recon_z = LLOYD_LEVELS[codes]\n",
    "    # Efficient scoring: fold stds into query\n",
    "    q_scaled = queries * stds\n",
    "    scores = q_scaled @ recon_z.T\n",
    "    return scores\n",
    "\n",
    "def universal_search(queries, corpus, dim):\n",
    "    \"\"\"Score queries against universal (calibration-free) quantized corpus.\"\"\"\n",
    "    sigma_est = 1.0 / np.sqrt(dim)\n",
    "    bounds = LLOYD_BOUNDS * sigma_est\n",
    "    levels = LLOYD_LEVELS * sigma_est\n",
    "    codes = np.digitize(corpus, bounds).astype(np.uint8)\n",
    "    recon = levels[codes]\n",
    "    scores = queries @ recon.T\n",
    "    return scores\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Q15a: CROSS-CORPUS CALIBRATION STABILITY\")\n",
    "print(\"Calibrate Lloyd-Max on corpus A, evaluate on corpus B (same model)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# We'll test on the full-dim and one truncated dim for each model\n",
    "cross_results = []\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    full_dim = MODELS[model_key].dim\n",
    "    dims_to_test = [full_dim]\n",
    "    # Add one truncated dim if model supports Matryoshka\n",
    "    mat_dims = MODELS[model_key].matryoshka_dims\n",
    "    if len(mat_dims) > 1:\n",
    "        dims_to_test.append(256)\n",
    "    \n",
    "    for dim in dims_to_test:\n",
    "        # Load both datasets for this model\n",
    "        datasets_available = []\n",
    "        for ds in DATASETS_TO_TEST:\n",
    "            if (model_key, ds) in data:\n",
    "                datasets_available.append(ds)\n",
    "        \n",
    "        if len(datasets_available) < 2:\n",
    "            continue\n",
    "        \n",
    "        for eval_ds in datasets_available:\n",
    "            d_eval = data[(model_key, eval_ds)]\n",
    "            corpus_eval = truncate_and_normalize(d_eval[\"corpus_emb\"], dim)\n",
    "            queries_eval = truncate_and_normalize(d_eval[\"query_emb\"], dim)\n",
    "            \n",
    "            # Float32 baseline\n",
    "            scores_f32 = queries_eval @ corpus_eval.T\n",
    "            idx_f32 = search_topk(scores_f32, 100)\n",
    "            m_f32 = evaluate_indices(idx_f32, d_eval[\"qrels\"], d_eval[\"doc_id_to_idx\"], \n",
    "                                     d_eval[\"query_ids\"], k=10)\n",
    "            \n",
    "            # Self-calibrated Lloyd-Max (normal case)\n",
    "            med_self = np.median(corpus_eval, axis=0)\n",
    "            std_self = np.std(corpus_eval, axis=0)\n",
    "            scores_self = lloyd_max_search(queries_eval, corpus_eval, med_self, std_self)\n",
    "            idx_self = search_topk(scores_self, 100)\n",
    "            m_self = evaluate_indices(idx_self, d_eval[\"qrels\"], d_eval[\"doc_id_to_idx\"],\n",
    "                                      d_eval[\"query_ids\"], k=10)\n",
    "            \n",
    "            # Cross-calibrated: use stats from OTHER dataset\n",
    "            for calib_ds in datasets_available:\n",
    "                if calib_ds == eval_ds:\n",
    "                    continue\n",
    "                d_calib = data[(model_key, calib_ds)]\n",
    "                corpus_calib = truncate_and_normalize(d_calib[\"corpus_emb\"], dim)\n",
    "                med_cross = np.median(corpus_calib, axis=0)\n",
    "                std_cross = np.std(corpus_calib, axis=0)\n",
    "                \n",
    "                scores_cross = lloyd_max_search(queries_eval, corpus_eval, med_cross, std_cross)\n",
    "                idx_cross = search_topk(scores_cross, 100)\n",
    "                m_cross = evaluate_indices(idx_cross, d_eval[\"qrels\"], d_eval[\"doc_id_to_idx\"],\n",
    "                                           d_eval[\"query_ids\"], k=10)\n",
    "                \n",
    "                # Universal (calibration-free) for comparison\n",
    "                scores_uni = universal_search(queries_eval, corpus_eval, dim)\n",
    "                idx_uni = search_topk(scores_uni, 100)\n",
    "                m_uni = evaluate_indices(idx_uni, d_eval[\"qrels\"], d_eval[\"doc_id_to_idx\"],\n",
    "                                         d_eval[\"query_ids\"], k=10)\n",
    "                \n",
    "                row = {\n",
    "                    \"model\": model_key, \"eval_ds\": eval_ds, \"calib_ds\": calib_ds, \"dim\": dim,\n",
    "                    \"float32\": m_f32[\"ndcg@10\"], \"self_calib\": m_self[\"ndcg@10\"],\n",
    "                    \"cross_calib\": m_cross[\"ndcg@10\"], \"universal\": m_uni[\"ndcg@10\"],\n",
    "                }\n",
    "                cross_results.append(row)\n",
    "                \n",
    "                # How similar are the calibration stats?\n",
    "                med_corr = np.corrcoef(med_self, med_cross)[0, 1]\n",
    "                std_corr = np.corrcoef(std_self, std_cross)[0, 1]\n",
    "                med_rmse = np.sqrt(np.mean((med_self - med_cross)**2))\n",
    "                std_rmse = np.sqrt(np.mean((std_self - std_cross)**2))\n",
    "                \n",
    "                print(f\"\\n{model_key} d={dim} | eval={eval_ds}, calib={calib_ds}\")\n",
    "                print(f\"  Stat similarity: median_corr={med_corr:.4f}, std_corr={std_corr:.4f}\")\n",
    "                print(f\"  Stat RMSE:       median={med_rmse:.6f}, std={std_rmse:.6f}\")\n",
    "                print(f\"  Float32:         {row['float32']:.4f}\")\n",
    "                print(f\"  Self-calibrated: {row['self_calib']:.4f}\")\n",
    "                print(f\"  Cross-calib:     {row['cross_calib']:.4f} (Δ vs self: {row['cross_calib'] - row['self_calib']:+.4f})\")\n",
    "                print(f\"  Universal:       {row['universal']:.4f} (Δ vs self: {row['universal'] - row['self_calib']:+.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"─\" * 80)\n",
    "print(\"SUMMARY: Cross-calibration degradation\")\n",
    "print(f\"{'Model':<25} {'Dim':>5} {'Eval→Calib':<25} {'Self':>7} {'Cross':>7} {'Univ':>7} {'Cross−Self':>11}\")\n",
    "print(\"─\" * 80)\n",
    "for r in cross_results:\n",
    "    label = f\"{r['eval_ds']}→{r['calib_ds']}\"\n",
    "    delta = r['cross_calib'] - r['self_calib']\n",
    "    print(f\"{r['model']:<25} {r['dim']:>5} {label:<25} {r['self_calib']:>7.4f} {r['cross_calib']:>7.4f} {r['universal']:>7.4f} {delta:>+11.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8792a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Q15b: Small-Sample Calibration — How many vectors are enough?\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "#\n",
    "# In a streaming setting, you might only have N documents when you first create\n",
    "# the index. How quickly do the calibration stats converge?\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Q15b: SMALL-SAMPLE CALIBRATION CONVERGENCE\")\n",
    "print(\"How many vectors are needed for stable Lloyd-Max calibration?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_sizes = [10, 50, 100, 500, 1000, 2000, 5000]\n",
    "sample_results = []\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    full_dim = MODELS[model_key].dim\n",
    "    dims_to_test = [full_dim]\n",
    "    if len(MODELS[model_key].matryoshka_dims) > 1:\n",
    "        dims_to_test.append(256)\n",
    "    \n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        \n",
    "        d = data[(model_key, dataset)]\n",
    "        n_corpus = d[\"corpus_emb\"].shape[0]\n",
    "        \n",
    "        for dim in dims_to_test:\n",
    "            corpus = truncate_and_normalize(d[\"corpus_emb\"], dim)\n",
    "            queries = truncate_and_normalize(d[\"query_emb\"], dim)\n",
    "            \n",
    "            # Full-corpus calibration (ground truth)\n",
    "            med_full = np.median(corpus, axis=0)\n",
    "            std_full = np.std(corpus, axis=0)\n",
    "            \n",
    "            scores_full = lloyd_max_search(queries, corpus, med_full, std_full)\n",
    "            idx_full = search_topk(scores_full, 100)\n",
    "            m_full = evaluate_indices(idx_full, d[\"qrels\"], d[\"doc_id_to_idx\"],\n",
    "                                      d[\"query_ids\"], k=10)\n",
    "            \n",
    "            # Universal baseline\n",
    "            scores_uni = universal_search(queries, corpus, dim)\n",
    "            idx_uni = search_topk(scores_uni, 100)\n",
    "            m_uni = evaluate_indices(idx_uni, d[\"qrels\"], d[\"doc_id_to_idx\"],\n",
    "                                     d[\"query_ids\"], k=10)\n",
    "            \n",
    "            print(f\"\\n{model_key} / {dataset} d={dim} (n_corpus={n_corpus})\")\n",
    "            print(f\"  Full-calib Lloyd-Max: {m_full['ndcg@10']:.4f}\")\n",
    "            print(f\"  Universal (calib-free): {m_uni['ndcg@10']:.4f}\")\n",
    "            print(f\"  {'N':>6} | {'NDCG@10':>8} | {'Δ vs full':>10} | {'med_corr':>9} | {'std_corr':>9}\")\n",
    "            print(f\"  {'─'*55}\")\n",
    "            \n",
    "            for n in sample_sizes:\n",
    "                if n > n_corpus:\n",
    "                    continue\n",
    "                \n",
    "                # Average over 5 random subsamples for stability\n",
    "                ndcgs = []\n",
    "                med_corrs = []\n",
    "                std_corrs = []\n",
    "                for trial in range(5):\n",
    "                    rng = np.random.RandomState(42 + trial)\n",
    "                    idx_sample = rng.choice(n_corpus, min(n, n_corpus), replace=False)\n",
    "                    sample = corpus[idx_sample]\n",
    "                    \n",
    "                    med_n = np.median(sample, axis=0)\n",
    "                    std_n = np.std(sample, axis=0)\n",
    "                    \n",
    "                    scores_n = lloyd_max_search(queries, corpus, med_n, std_n)\n",
    "                    idx_n = search_topk(scores_n, 100)\n",
    "                    m_n = evaluate_indices(idx_n, d[\"qrels\"], d[\"doc_id_to_idx\"],\n",
    "                                           d[\"query_ids\"], k=10)\n",
    "                    \n",
    "                    ndcgs.append(m_n[\"ndcg@10\"])\n",
    "                    med_corrs.append(np.corrcoef(med_full, med_n)[0, 1])\n",
    "                    std_corrs.append(np.corrcoef(std_full, std_n)[0, 1])\n",
    "                \n",
    "                mean_ndcg = np.mean(ndcgs)\n",
    "                delta = mean_ndcg - m_full[\"ndcg@10\"]\n",
    "                print(f\"  {n:>6} | {mean_ndcg:>8.4f} | {delta:>+10.4f} | {np.mean(med_corrs):>9.4f} | {np.mean(std_corrs):>9.4f}\")\n",
    "                \n",
    "                sample_results.append({\n",
    "                    \"model\": model_key, \"dataset\": dataset, \"dim\": dim, \"n\": n,\n",
    "                    \"ndcg_mean\": mean_ndcg, \"ndcg_std\": np.std(ndcgs),\n",
    "                    \"ndcg_full\": m_full[\"ndcg@10\"], \"ndcg_universal\": m_uni[\"ndcg@10\"],\n",
    "                    \"delta_vs_full\": delta,\n",
    "                    \"med_corr\": np.mean(med_corrs), \"std_corr\": np.mean(std_corrs),\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Q15c: Synthetic Drift — How badly does stale calibration hurt?\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "#\n",
    "# Simulate what happens when the corpus distribution shifts after calibration:\n",
    "# 1. Per-dim bias drift: add a random bias to each dimension\n",
    "# 2. Variance scaling: scale some dimensions up/down\n",
    "# 3. Topic drift: mix in embeddings from a different domain\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Q15c: SYNTHETIC DRIFT SIMULATION\")\n",
    "print(\"What happens when calibration becomes stale?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# For each model/dataset, calibrate on original corpus, then:\n",
    "# (a) Perturb ALL corpus vectors by adding per-dim bias (simulating domain shift)\n",
    "# (b) Perturb ALL corpus vectors by scaling per-dim std\n",
    "# Then measure how Lloyd-Max (with stale stats) compares to:\n",
    "#   - Lloyd-Max with fresh stats\n",
    "#   - Universal quantizer (unaffected by drift)\n",
    "\n",
    "drift_strengths = [0.0, 0.25, 0.5, 1.0, 2.0, 3.0]  # multiples of natural per-dim std\n",
    "drift_results = []\n",
    "\n",
    "for model_key in [\"mxbai-embed-large-v1\"]:  # Focus on one model for clarity\n",
    "    for dataset in [\"scifact\"]:\n",
    "        if (model_key, dataset) not in data:\n",
    "            continue\n",
    "        \n",
    "        dd = data[(model_key, dataset)]\n",
    "        full_dim = MODELS[model_key].dim\n",
    "        corpus_orig = truncate_and_normalize(dd[\"corpus_emb\"], full_dim)\n",
    "        queries = truncate_and_normalize(dd[\"query_emb\"], full_dim)\n",
    "        \n",
    "        # Calibrate on ORIGINAL corpus\n",
    "        med_orig = np.median(corpus_orig, axis=0)\n",
    "        std_orig = np.std(corpus_orig, axis=0)\n",
    "        \n",
    "        # Float32 baseline (no quantization, no drift — ground truth)\n",
    "        scores_f32 = queries @ corpus_orig.T\n",
    "        idx_f32 = search_topk(scores_f32, 100)\n",
    "        m_f32_orig = evaluate_indices(idx_f32, dd[\"qrels\"], dd[\"doc_id_to_idx\"],\n",
    "                                       dd[\"query_ids\"], k=10)\n",
    "        \n",
    "        print(f\"\\n{model_key} / {dataset} d={full_dim}\")\n",
    "        print(f\"Original float32: {m_f32_orig['ndcg@10']:.4f}\")\n",
    "        print(f\"\\n--- Bias Drift (add random per-dim offset) ---\")\n",
    "        print(f\"{'Drift':>8} | {'F32+drift':>10} | {'LM(stale)':>10} | {'LM(fresh)':>10} | {'Universal':>10} | {'Stale−Fresh':>12}\")\n",
    "        print(f\"{'─'*75}\")\n",
    "        \n",
    "        rng = np.random.RandomState(42)\n",
    "        # Generate a fixed random direction for the drift\n",
    "        drift_direction = rng.randn(full_dim).astype(np.float32)\n",
    "        drift_direction /= np.linalg.norm(drift_direction)  # unit vector\n",
    "        \n",
    "        for strength in drift_strengths:\n",
    "            # Apply bias drift: shift corpus by strength * per-dim std * random direction\n",
    "            bias = strength * std_orig * drift_direction\n",
    "            corpus_drifted = corpus_orig + bias\n",
    "            # Re-normalize to maintain L2 unit norm (crucial!)\n",
    "            norms = np.linalg.norm(corpus_drifted, axis=1, keepdims=True)\n",
    "            corpus_drifted = corpus_drifted / np.where(norms == 0, 1.0, norms)\n",
    "            \n",
    "            # Float32 on drifted corpus (no quantization — but note rankings change!)\n",
    "            scores_f32d = queries @ corpus_drifted.T\n",
    "            idx_f32d = search_topk(scores_f32d, 100)\n",
    "            m_f32d = evaluate_indices(idx_f32d, dd[\"qrels\"], dd[\"doc_id_to_idx\"],\n",
    "                                      dd[\"query_ids\"], k=10)\n",
    "            \n",
    "            # Lloyd-Max with STALE calibration (original stats)\n",
    "            scores_stale = lloyd_max_search(queries, corpus_drifted, med_orig, std_orig)\n",
    "            idx_stale = search_topk(scores_stale, 100)\n",
    "            m_stale = evaluate_indices(idx_stale, dd[\"qrels\"], dd[\"doc_id_to_idx\"],\n",
    "                                        dd[\"query_ids\"], k=10)\n",
    "            \n",
    "            # Lloyd-Max with FRESH calibration (re-calibrated on drifted corpus)\n",
    "            med_fresh = np.median(corpus_drifted, axis=0)\n",
    "            std_fresh = np.std(corpus_drifted, axis=0)\n",
    "            scores_fresh = lloyd_max_search(queries, corpus_drifted, med_fresh, std_fresh)\n",
    "            idx_fresh = search_topk(scores_fresh, 100)\n",
    "            m_fresh = evaluate_indices(idx_fresh, dd[\"qrels\"], dd[\"doc_id_to_idx\"],\n",
    "                                        dd[\"query_ids\"], k=10)\n",
    "            \n",
    "            # Universal (calibration-free)\n",
    "            scores_univ = universal_search(queries, corpus_drifted, full_dim)\n",
    "            idx_univ = search_topk(scores_univ, 100)\n",
    "            m_univ = evaluate_indices(idx_univ, dd[\"qrels\"], dd[\"doc_id_to_idx\"],\n",
    "                                      dd[\"query_ids\"], k=10)\n",
    "            \n",
    "            stale_gap = m_stale[\"ndcg@10\"] - m_fresh[\"ndcg@10\"]\n",
    "            print(f\"{strength:>8.2f} | {m_f32d['ndcg@10']:>10.4f} | {m_stale['ndcg@10']:>10.4f} | {m_fresh['ndcg@10']:>10.4f} | {m_univ['ndcg@10']:>10.4f} | {stale_gap:>+12.4f}\")\n",
    "            \n",
    "            drift_results.append({\n",
    "                \"type\": \"bias\", \"strength\": strength,\n",
    "                \"f32\": m_f32d[\"ndcg@10\"], \"stale\": m_stale[\"ndcg@10\"],\n",
    "                \"fresh\": m_fresh[\"ndcg@10\"], \"universal\": m_univ[\"ndcg@10\"],\n",
    "            })\n",
    "        \n",
    "        # --- Variance drift: scale per-dim std by a factor ---\n",
    "        print(f\"\\n--- Variance Drift (scale per-dim variance) ---\")\n",
    "        var_factors = [0.5, 0.75, 1.0, 1.25, 1.5, 2.0]\n",
    "        print(f\"{'Factor':>8} | {'F32+drift':>10} | {'LM(stale)':>10} | {'LM(fresh)':>10} | {'Universal':>10} | {'Stale−Fresh':>12}\")\n",
    "        print(f\"{'─'*75}\")\n",
    "        \n",
    "        for factor in var_factors:\n",
    "            # Scale each dim's deviation from median by factor\n",
    "            centered = corpus_orig - med_orig\n",
    "            corpus_scaled = med_orig + centered * factor\n",
    "            # Re-normalize\n",
    "            norms = np.linalg.norm(corpus_scaled, axis=1, keepdims=True)\n",
    "            corpus_scaled = corpus_scaled / np.where(norms == 0, 1.0, norms)\n",
    "            \n",
    "            scores_f32s = queries @ corpus_scaled.T\n",
    "            idx_f32s = search_topk(scores_f32s, 100)\n",
    "            m_f32s = evaluate_indices(idx_f32s, dd[\"qrels\"], dd[\"doc_id_to_idx\"],\n",
    "                                      dd[\"query_ids\"], k=10)\n",
    "            \n",
    "            scores_stale = lloyd_max_search(queries, corpus_scaled, med_orig, std_orig)\n",
    "            idx_stale = search_topk(scores_stale, 100)\n",
    "            m_stale = evaluate_indices(idx_stale, dd[\"qrels\"], dd[\"doc_id_to_idx\"],\n",
    "                                        dd[\"query_ids\"], k=10)\n",
    "            \n",
    "            med_fresh = np.median(corpus_scaled, axis=0)\n",
    "            std_fresh = np.std(corpus_scaled, axis=0)\n",
    "            scores_fresh = lloyd_max_search(queries, corpus_scaled, med_fresh, std_fresh)\n",
    "            idx_fresh = search_topk(scores_fresh, 100)\n",
    "            m_fresh = evaluate_indices(idx_fresh, dd[\"qrels\"], dd[\"doc_id_to_idx\"],\n",
    "                                        dd[\"query_ids\"], k=10)\n",
    "            \n",
    "            scores_univ = universal_search(queries, corpus_scaled, full_dim)\n",
    "            idx_univ = search_topk(scores_univ, 100)\n",
    "            m_univ = evaluate_indices(idx_univ, dd[\"qrels\"], dd[\"doc_id_to_idx\"],\n",
    "                                      dd[\"query_ids\"], k=10)\n",
    "            \n",
    "            stale_gap = m_stale[\"ndcg@10\"] - m_fresh[\"ndcg@10\"]\n",
    "            print(f\"{factor:>8.2f} | {m_f32s['ndcg@10']:>10.4f} | {m_stale['ndcg@10']:>10.4f} | {m_fresh['ndcg@10']:>10.4f} | {m_univ['ndcg@10']:>10.4f} | {stale_gap:>+12.4f}\")\n",
    "            \n",
    "            drift_results.append({\n",
    "                \"type\": \"variance\", \"strength\": factor,\n",
    "                \"f32\": m_f32s[\"ndcg@10\"], \"stale\": m_stale[\"ndcg@10\"],\n",
    "                \"fresh\": m_fresh[\"ndcg@10\"], \"universal\": m_univ[\"ndcg@10\"],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Q15d: Visualization — Small-sample convergence + drift plots\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(\"Streaming Calibration: Key Results\", fontweight=\"bold\", fontsize=13)\n",
    "\n",
    "# --- Plot 1: Small-sample convergence ---\n",
    "ax = axes[0]\n",
    "ax.set_title(\"Small-Sample Calibration Convergence\")\n",
    "\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        full_dim = MODELS[model_key].dim\n",
    "        rows = [r for r in sample_results \n",
    "                if r[\"model\"] == model_key and r[\"dataset\"] == dataset and r[\"dim\"] == full_dim]\n",
    "        if not rows:\n",
    "            continue\n",
    "        ns = [r[\"n\"] for r in rows]\n",
    "        ndcgs = [r[\"ndcg_mean\"] for r in rows]\n",
    "        full_ndcg = rows[0][\"ndcg_full\"]\n",
    "        uni_ndcg = rows[0][\"ndcg_universal\"]\n",
    "        \n",
    "        short = model_key.split(\"-\")[0]\n",
    "        ax.plot(ns, ndcgs, \"o-\", label=f\"{short}/{dataset} (full={full_ndcg:.3f})\", markersize=4)\n",
    "        ax.axhline(y=full_ndcg, color=\"gray\", linestyle=\":\", alpha=0.3)\n",
    "\n",
    "ax.set_xlabel(\"Calibration sample size\")\n",
    "ax.set_ylabel(\"NDCG@10\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.legend(fontsize=7)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 2: Bias drift ---\n",
    "ax = axes[1]\n",
    "ax.set_title(\"Bias Drift Robustness\")\n",
    "\n",
    "bias_rows = [r for r in drift_results if r[\"type\"] == \"bias\"]\n",
    "if bias_rows:\n",
    "    strengths = [r[\"strength\"] for r in bias_rows]\n",
    "    ax.plot(strengths, [r[\"f32\"] for r in bias_rows], \"o-\", color=\"#2ecc71\", label=\"Float32\", alpha=0.8)\n",
    "    ax.plot(strengths, [r[\"fresh\"] for r in bias_rows], \"s-\", color=\"#2c3e50\", label=\"LM (fresh calib)\")\n",
    "    ax.plot(strengths, [r[\"stale\"] for r in bias_rows], \"^--\", color=\"#e74c3c\", label=\"LM (stale calib)\")\n",
    "    ax.plot(strengths, [r[\"universal\"] for r in bias_rows], \"v:\", color=\"#3498db\", label=\"Universal (calib-free)\")\n",
    "    ax.set_xlabel(\"Drift strength (× per-dim σ)\")\n",
    "    ax.set_ylabel(\"NDCG@10\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 3: Variance drift ---\n",
    "ax = axes[2]\n",
    "ax.set_title(\"Variance Drift Robustness\")\n",
    "\n",
    "var_rows = [r for r in drift_results if r[\"type\"] == \"variance\"]\n",
    "if var_rows:\n",
    "    factors = [r[\"strength\"] for r in var_rows]\n",
    "    ax.plot(factors, [r[\"f32\"] for r in var_rows], \"o-\", color=\"#2ecc71\", label=\"Float32\", alpha=0.8)\n",
    "    ax.plot(factors, [r[\"fresh\"] for r in var_rows], \"s-\", color=\"#2c3e50\", label=\"LM (fresh calib)\")\n",
    "    ax.plot(factors, [r[\"stale\"] for r in var_rows], \"^--\", color=\"#e74c3c\", label=\"LM (stale calib)\")\n",
    "    ax.plot(factors, [r[\"universal\"] for r in var_rows], \"v:\", color=\"#3498db\", label=\"Universal (calib-free)\")\n",
    "    ax.set_xlabel(\"Variance scale factor\")\n",
    "    ax.set_ylabel(\"NDCG@10\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ── Final summary ──\n",
    "print(\"=\" * 80)\n",
    "print(\"Q15 SUMMARY: STREAMING CALIBRATION RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. CROSS-CORPUS CALIBRATION:\")\n",
    "for r in cross_results:\n",
    "    delta = r[\"cross_calib\"] - r[\"self_calib\"]\n",
    "    uni_delta = r[\"universal\"] - r[\"self_calib\"]\n",
    "    verdict = \"✓ VIABLE\" if abs(delta) < 0.01 else (\"≈ OK\" if abs(delta) < 0.03 else \"✗ RISKY\")\n",
    "    print(f\"   {r['model'][:6]} d={r['dim']:>4} {r['eval_ds']}→{r['calib_ds']}: \"\n",
    "          f\"cross Δ={delta:+.4f} {verdict}  |  universal Δ={uni_delta:+.4f}\")\n",
    "\n",
    "print(\"\\n2. SMALL-SAMPLE CONVERGENCE:\")\n",
    "for model_key in MODELS_TO_TEST:\n",
    "    for dataset in DATASETS_TO_TEST:\n",
    "        full_dim = MODELS[model_key].dim\n",
    "        rows = [r for r in sample_results \n",
    "                if r[\"model\"] == model_key and r[\"dataset\"] == dataset and r[\"dim\"] == full_dim]\n",
    "        if not rows:\n",
    "            continue\n",
    "        # Find smallest N where delta < 0.005\n",
    "        min_n = \"never\"\n",
    "        for r in rows:\n",
    "            if abs(r[\"delta_vs_full\"]) < 0.005:\n",
    "                min_n = str(r[\"n\"])\n",
    "                break\n",
    "        print(f\"   {model_key[:6]} / {dataset}: converges within 0.005 at N={min_n}\")\n",
    "\n",
    "print(\"\\n3. DRIFT ROBUSTNESS:\")\n",
    "if bias_rows:\n",
    "    # Find max drift where stale is still within 0.01 of fresh\n",
    "    max_safe = 0.0\n",
    "    for r in bias_rows:\n",
    "        if abs(r[\"stale\"] - r[\"fresh\"]) < 0.01:\n",
    "            max_safe = r[\"strength\"]\n",
    "    print(f\"   Bias drift: stale calibration safe up to {max_safe:.1f}× per-dim σ\")\n",
    "    \n",
    "    # Check if universal survives drift better\n",
    "    worst_stale = max(abs(r[\"stale\"] - r[\"fresh\"]) for r in bias_rows)\n",
    "    worst_uni_vs_fresh = max(abs(r[\"universal\"] - r[\"fresh\"]) for r in bias_rows)\n",
    "    print(f\"   Worst stale degradation: {worst_stale:.4f}\")\n",
    "    print(f\"   Worst universal gap vs fresh: {worst_uni_vs_fresh:.4f}\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDATION for Vespa:\")\n",
    "print(\"   → At full dim: Use Universal Quantizer (calibration-free)\")\n",
    "print(\"   → At truncated dim: Calibrate once on 500-1000 vectors from any representative sample\")\n",
    "print(\"   → Cross-corpus calibration is viable: stats are model-intrinsic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
